{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DQN_Agent_T2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemantTiwariGitHub/RLCabs/blob/main/DQN_Agent_T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CseJVoBt8jWJ"
      },
      "source": [
        "### Cab-Driver Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-nxS9L8jWK"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import deque\n",
        "import collections\n",
        "import pickle\n",
        "import time \n",
        "\n",
        "# for building DQN model\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# for plotting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the environment\n",
        "from Env import CabDriver"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPZChKPx8jWO"
      },
      "source": [
        "#### Defining Time Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fppApKKo8jWP"
      },
      "source": [
        "# Loading the time matrix provided\n",
        "Time_matrix = np.load(\"TM.npy\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DBeLhG48jWc"
      },
      "source": [
        "#Defining a function to save the Q-dictionary as a pickle file\n",
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjaZhYb28jWe"
      },
      "source": [
        "### Agent Class\n",
        "\n",
        "If you are using this framework, you need to fill the following to complete the following code block:\n",
        "1. State and Action Size\n",
        "2. Hyperparameters\n",
        "3. Create a neural-network model in function 'build_model()'\n",
        "4. Define epsilon-greedy strategy in function 'get_action()'\n",
        "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
        "6. Complete the 'train_model()' function with following logic:\n",
        "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
        "      - Initialise your input and output batch for training the model\n",
        "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
        "      - Get Q(s', a) values from the last trained model\n",
        "      - Update the input batch as your encoded state and output batch as your Q-values\n",
        "      - Then fit your DQN model using the updated input and output batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuqIwf-O8jWf"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # Define size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Write here: Specify you hyper parameters for the DQN\n",
        "        self.discount_factor = 0.95\n",
        "        self.learning_rate = 0.01       \n",
        "        self.epsilon = 1\n",
        "        self.epsilon_max = 1\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.epsilon_min = 0.01\n",
        "        \n",
        "        self.batch_size = 32        \n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=2000)\n",
        "   \n",
        "        self.states_tracked = []\n",
        "        self.track_state = np.array(env.state_encod_arch1([1,2,1])).reshape(1, self.state_size)\n",
        "        \n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        # Write your code here: Add layers to your neural nets   \n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "        # the output layer: output is of size num_actions\n",
        "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
        "        model.summary\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    def get_action(self, state, possible_actions_index, actions):\n",
        "        #print(state)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # explore: choose a random action from all possible actions\n",
        "            randAction = random.choice(possible_actions_index)\n",
        "            return randAction\n",
        "        else:\n",
        "            # choose the action with the highest q(s, a)\n",
        "            # the first index corresponds to the batch size, so\n",
        "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
        "            encodedState = env.state_encod_arch1(state)\n",
        "            batchEncodedState = np.array(encodedState).reshape(1, 36)\n",
        "            q_value = self.model.predict(batchEncodedState)\n",
        "            possibleValues = [q_value[0][i] for i in possible_actions_index]\n",
        "            return possible_actions_index[np.argmax(possibleValues)]\n",
        "\n",
        "          \n",
        "         \n",
        "   \n",
        "\n",
        "    def append_sample(self, state, action, reward, next_state,done):\n",
        "    # Write your code here:\n",
        "    # save sample <s,a,r,s'> to the replay memory\n",
        "      self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    \n",
        "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
        "    def train_model(self):\n",
        "        \n",
        "        if len(self.memory) > self.batch_size:\n",
        "            \n",
        "            # sample minibatch from memory\n",
        "            minibatch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "            # initialise two matrices - update_input and update_output\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\n",
        "            actions, rewards, done = [], [], []\n",
        "\n",
        "            # populate update_input and update_output and the lists rewards, actions, done\n",
        "            for i in range(self.batch_size):\n",
        "                state, action, reward, next_state, done_boolean = minibatch[i]\n",
        "                update_input[i] = env.state_encod_arch1(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                update_output[i] = env.state_encod_arch1(next_state)\n",
        "                done.append(done_boolean)\n",
        "\n",
        "            # predict the target q-values from states s\n",
        "            target = self.model.predict(update_input)\n",
        "\n",
        "            # target for q-network\n",
        "            target_qval = self.model.predict(update_output)\n",
        "\n",
        "            # update the target values\n",
        "            for i in range(self.batch_size):\n",
        "                if done[i]:\n",
        "                    target[i][actions[i]] = rewards[i]\n",
        "                else: # non-terminal state\n",
        "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
        "\n",
        "            # model fit\n",
        "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)\n",
        "\n",
        "    def save_tracking_states(self):\n",
        "        q_value = self.model.predict(self.track_state)\n",
        "        self.states_tracked.append(q_value[0][5])\n",
        "\n",
        "    def savePickle(self, name):\n",
        "        with open(name, 'wb') as file:  \n",
        "          pickle.dump(self.model, file,pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0GAuc0j8jWj"
      },
      "source": [
        "episodeTime = 24*30 \n",
        "Episodes = 1000\n",
        "\n",
        "env = CabDriver()\n",
        "action_space, state_space, state = env.reset()\n",
        "\n",
        "state_size = 36\n",
        "action_size = len(action_space)\n",
        "\n",
        "\n",
        "agent = DQNAgent(state_size=state_size,action_size=action_size)\n",
        "\n",
        "\n",
        "rewards_per_episode, episodes = [], []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNE467Wi8jWm"
      },
      "source": [
        "### DQN block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPsqSz7W8jWm",
        "outputId": "605bf57a-5bc4-4bfa-9213-a061ad110b6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "startTime = time.time()\n",
        "scoreTracked = []\n",
        "#### simulation starts ####\n",
        "for episode in range(Episodes):\n",
        "\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    # reset at the start of each episode\n",
        "    env = CabDriver()\n",
        "    action_space, state_space, state = env.reset()\n",
        "    \n",
        "    currentEpisodeTime=0\n",
        "    maxEpisodeTime = episodeTime \n",
        "   \n",
        "\n",
        "\n",
        "    while not done:\n",
        "        # get action for the current state and take a step in the environment\n",
        "        possible_actions_index, actions = env.requests(state)\n",
        "      \n",
        "        #print(state)\n",
        "        #print(actions)\n",
        "        #print(possible_actions_index)\n",
        "        \n",
        "\n",
        "        action = agent.get_action(state, possible_actions_index, actions)\n",
        "\n",
        "        #print(action)\n",
        "\n",
        "\n",
        "        next_state, reward, timePassed = env.next_state_func(state, env.action_space[action], Time_matrix)\n",
        "        currentEpisodeTime = currentEpisodeTime + timePassed\n",
        "\n",
        "        if (currentEpisodeTime > maxEpisodeTime):\n",
        "          done = True\n",
        "  \n",
        "        # save the sample <s, a, r, s', done> to the replay memory\n",
        "        agent.append_sample(state, action, reward, next_state, done)\n",
        "      \n",
        "\n",
        "        # train after each step\n",
        "        agent.train_model()\n",
        "\n",
        "        # add reward to the total score of this episode\n",
        "        score += reward\n",
        "\n",
        " \n",
        "        #print(i,\": \" , state , \" :  \" , action , \" :  \" , reward , \" : \" , next_state , \" : \" , done, \" : \" , score)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "    # store total reward obtained in this episode\n",
        "    rewards_per_episode.append(score)\n",
        "    episodes.append(episode)\n",
        "\n",
        "    # epsilon decay\n",
        "    if agent.epsilon > agent.epsilon_min:\n",
        "          agent.epsilon = agent.epsilon_min + (agent.epsilon_max - agent.epsilon_min) * np.exp(-0.0005 * episode)\n",
        "\n",
        "    # every episode:\n",
        "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}\".format(episode,\n",
        "                                                                         score,\n",
        "                                                                         len(agent.memory),\n",
        "                                                                         agent.epsilon))\n",
        "    \n",
        "    if (((episode +1) % 10) == 0):\n",
        "       agent.save_tracking_states()\n",
        "    \n",
        "    if (((episode +1 )% 500) == 0):\n",
        "      agent.save(name=\"model_weights.h5\")\n",
        "\n",
        "#### simulation complete ####\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0, reward -129, memory_length 160, epsilon 1.0\n",
            "episode 1, reward 116, memory_length 292, epsilon 0.9995051237293776\n",
            "episode 2, reward -344, memory_length 425, epsilon 0.9990104948350412\n",
            "episode 3, reward -205, memory_length 571, epsilon 0.9985161131933338\n",
            "episode 4, reward -150, memory_length 713, epsilon 0.9980219786806598\n",
            "episode 5, reward -448, memory_length 856, epsilon 0.9975280911734855\n",
            "episode 6, reward -138, memory_length 997, epsilon 0.9970344505483393\n",
            "episode 7, reward -160, memory_length 1135, epsilon 0.9965410566818107\n",
            "episode 8, reward -331, memory_length 1285, epsilon 0.9960479094505515\n",
            "episode 9, reward -194, memory_length 1433, epsilon 0.9955550087312749\n",
            "episode 10, reward -177, memory_length 1568, epsilon 0.9950623544007555\n",
            "episode 11, reward -61, memory_length 1721, epsilon 0.9945699463358298\n",
            "episode 12, reward 22, memory_length 1846, epsilon 0.9940777844133959\n",
            "episode 13, reward -65, memory_length 1984, epsilon 0.9935858685104132\n",
            "episode 14, reward -287, memory_length 2000, epsilon 0.9930941985039028\n",
            "episode 15, reward -147, memory_length 2000, epsilon 0.992602774270947\n",
            "episode 16, reward -259, memory_length 2000, epsilon 0.99211159568869\n",
            "episode 17, reward -298, memory_length 2000, epsilon 0.991620662634337\n",
            "episode 18, reward -104, memory_length 2000, epsilon 0.9911299749851548\n",
            "episode 19, reward -330, memory_length 2000, epsilon 0.9906395326184715\n",
            "episode 20, reward -347, memory_length 2000, epsilon 0.9901493354116764\n",
            "episode 21, reward -368, memory_length 2000, epsilon 0.9896593832422202\n",
            "episode 22, reward 192, memory_length 2000, epsilon 0.9891696759876151\n",
            "episode 23, reward 7, memory_length 2000, epsilon 0.9886802135254339\n",
            "episode 24, reward -133, memory_length 2000, epsilon 0.9881909957333113\n",
            "episode 25, reward -15, memory_length 2000, epsilon 0.9877020224889427\n",
            "episode 26, reward -257, memory_length 2000, epsilon 0.9872132936700847\n",
            "episode 27, reward -57, memory_length 2000, epsilon 0.9867248091545553\n",
            "episode 28, reward 43, memory_length 2000, epsilon 0.9862365688202333\n",
            "episode 29, reward 157, memory_length 2000, epsilon 0.9857485725450585\n",
            "episode 30, reward -105, memory_length 2000, epsilon 0.985260820207032\n",
            "episode 31, reward -57, memory_length 2000, epsilon 0.9847733116842156\n",
            "episode 32, reward -217, memory_length 2000, epsilon 0.9842860468547323\n",
            "episode 33, reward -454, memory_length 2000, epsilon 0.9837990255967657\n",
            "episode 34, reward -120, memory_length 2000, epsilon 0.9833122477885605\n",
            "episode 35, reward -146, memory_length 2000, epsilon 0.9828257133084225\n",
            "episode 36, reward -341, memory_length 2000, epsilon 0.9823394220347178\n",
            "episode 37, reward -179, memory_length 2000, epsilon 0.9818533738458736\n",
            "episode 38, reward -79, memory_length 2000, epsilon 0.9813675686203779\n",
            "episode 39, reward -128, memory_length 2000, epsilon 0.9808820062367796\n",
            "episode 40, reward -389, memory_length 2000, epsilon 0.9803966865736877\n",
            "episode 41, reward -147, memory_length 2000, epsilon 0.9799116095097726\n",
            "episode 42, reward -127, memory_length 2000, epsilon 0.979426774923765\n",
            "episode 43, reward -397, memory_length 2000, epsilon 0.9789421826944561\n",
            "episode 44, reward -302, memory_length 2000, epsilon 0.978457832700698\n",
            "episode 45, reward -429, memory_length 2000, epsilon 0.977973724821403\n",
            "episode 46, reward 218, memory_length 2000, epsilon 0.9774898589355443\n",
            "episode 47, reward -155, memory_length 2000, epsilon 0.9770062349221554\n",
            "episode 48, reward -294, memory_length 2000, epsilon 0.9765228526603302\n",
            "episode 49, reward -176, memory_length 2000, epsilon 0.9760397120292232\n",
            "episode 50, reward -231, memory_length 2000, epsilon 0.9755568129080493\n",
            "episode 51, reward -55, memory_length 2000, epsilon 0.9750741551760836\n",
            "episode 52, reward -326, memory_length 2000, epsilon 0.9745917387126619\n",
            "episode 53, reward -37, memory_length 2000, epsilon 0.9741095633971797\n",
            "episode 54, reward -123, memory_length 2000, epsilon 0.9736276291090934\n",
            "episode 55, reward -281, memory_length 2000, epsilon 0.9731459357279194\n",
            "episode 56, reward 25, memory_length 2000, epsilon 0.9726644831332344\n",
            "episode 57, reward -415, memory_length 2000, epsilon 0.9721832712046753\n",
            "episode 58, reward -293, memory_length 2000, epsilon 0.9717022998219388\n",
            "episode 59, reward -204, memory_length 2000, epsilon 0.9712215688647823\n",
            "episode 60, reward 71, memory_length 2000, epsilon 0.970741078213023\n",
            "episode 61, reward -60, memory_length 2000, epsilon 0.9702608277465384\n",
            "episode 62, reward -420, memory_length 2000, epsilon 0.9697808173452657\n",
            "episode 63, reward -109, memory_length 2000, epsilon 0.9693010468892023\n",
            "episode 64, reward 42, memory_length 2000, epsilon 0.9688215162584056\n",
            "episode 65, reward 14, memory_length 2000, epsilon 0.968342225332993\n",
            "episode 66, reward -174, memory_length 2000, epsilon 0.9678631739931417\n",
            "episode 67, reward -291, memory_length 2000, epsilon 0.9673843621190888\n",
            "episode 68, reward -144, memory_length 2000, epsilon 0.9669057895911316\n",
            "episode 69, reward -113, memory_length 2000, epsilon 0.9664274562896267\n",
            "episode 70, reward -140, memory_length 2000, epsilon 0.9659493620949908\n",
            "episode 71, reward -398, memory_length 2000, epsilon 0.9654715068877004\n",
            "episode 72, reward 30, memory_length 2000, epsilon 0.9649938905482919\n",
            "episode 73, reward -239, memory_length 2000, epsilon 0.9645165129573607\n",
            "episode 74, reward -282, memory_length 2000, epsilon 0.9640393739955629\n",
            "episode 75, reward 25, memory_length 2000, epsilon 0.9635624735436136\n",
            "episode 76, reward -363, memory_length 2000, epsilon 0.9630858114822876\n",
            "episode 77, reward -114, memory_length 2000, epsilon 0.9626093876924194\n",
            "episode 78, reward -320, memory_length 2000, epsilon 0.962133202054903\n",
            "episode 79, reward -311, memory_length 2000, epsilon 0.9616572544506923\n",
            "episode 80, reward -268, memory_length 2000, epsilon 0.9611815447608\n",
            "episode 81, reward -182, memory_length 2000, epsilon 0.9607060728662988\n",
            "episode 82, reward -420, memory_length 2000, epsilon 0.9602308386483209\n",
            "episode 83, reward -78, memory_length 2000, epsilon 0.9597558419880575\n",
            "episode 84, reward -19, memory_length 2000, epsilon 0.9592810827667597\n",
            "episode 85, reward -97, memory_length 2000, epsilon 0.9588065608657375\n",
            "episode 86, reward -340, memory_length 2000, epsilon 0.9583322761663603\n",
            "episode 87, reward -146, memory_length 2000, epsilon 0.9578582285500573\n",
            "episode 88, reward 196, memory_length 2000, epsilon 0.9573844178983162\n",
            "episode 89, reward -328, memory_length 2000, epsilon 0.9569108440926846\n",
            "episode 90, reward -271, memory_length 2000, epsilon 0.9564375070147689\n",
            "episode 91, reward -354, memory_length 2000, epsilon 0.9559644065462349\n",
            "episode 92, reward 176, memory_length 2000, epsilon 0.9554915425688075\n",
            "episode 93, reward -293, memory_length 2000, epsilon 0.9550189149642706\n",
            "episode 94, reward -205, memory_length 2000, epsilon 0.9545465236144673\n",
            "episode 95, reward -121, memory_length 2000, epsilon 0.9540743684013001\n",
            "episode 96, reward -187, memory_length 2000, epsilon 0.9536024492067297\n",
            "episode 97, reward 20, memory_length 2000, epsilon 0.9531307659127765\n",
            "episode 98, reward -192, memory_length 2000, epsilon 0.9526593184015199\n",
            "episode 99, reward 94, memory_length 2000, epsilon 0.9521881065550976\n",
            "episode 100, reward -199, memory_length 2000, epsilon 0.9517171302557069\n",
            "episode 101, reward -186, memory_length 2000, epsilon 0.9512463893856036\n",
            "episode 102, reward 13, memory_length 2000, epsilon 0.9507758838271028\n",
            "episode 103, reward -202, memory_length 2000, epsilon 0.9503056134625776\n",
            "episode 104, reward -92, memory_length 2000, epsilon 0.9498355781744606\n",
            "episode 105, reward -125, memory_length 2000, epsilon 0.9493657778452432\n",
            "episode 106, reward -543, memory_length 2000, epsilon 0.9488962123574752\n",
            "episode 107, reward -85, memory_length 2000, epsilon 0.9484268815937651\n",
            "episode 108, reward 84, memory_length 2000, epsilon 0.9479577854367803\n",
            "episode 109, reward -456, memory_length 2000, epsilon 0.9474889237692468\n",
            "episode 110, reward -129, memory_length 2000, epsilon 0.947020296473949\n",
            "episode 111, reward 156, memory_length 2000, epsilon 0.9465519034337303\n",
            "episode 112, reward -76, memory_length 2000, epsilon 0.9460837445314924\n",
            "episode 113, reward 154, memory_length 2000, epsilon 0.9456158196501953\n",
            "episode 114, reward -185, memory_length 2000, epsilon 0.9451481286728581\n",
            "episode 115, reward -141, memory_length 2000, epsilon 0.944680671482558\n",
            "episode 116, reward -108, memory_length 2000, epsilon 0.9442134479624306\n",
            "episode 117, reward -132, memory_length 2000, epsilon 0.9437464579956699\n",
            "episode 118, reward 42, memory_length 2000, epsilon 0.9432797014655288\n",
            "episode 119, reward -321, memory_length 2000, epsilon 0.9428131782553177\n",
            "episode 120, reward 80, memory_length 2000, epsilon 0.9423468882484062\n",
            "episode 121, reward -200, memory_length 2000, epsilon 0.9418808313282216\n",
            "episode 122, reward -122, memory_length 2000, epsilon 0.9414150073782496\n",
            "episode 123, reward -80, memory_length 2000, epsilon 0.9409494162820343\n",
            "episode 124, reward -32, memory_length 2000, epsilon 0.940484057923178\n",
            "episode 125, reward -307, memory_length 2000, epsilon 0.940018932185341\n",
            "episode 126, reward 71, memory_length 2000, epsilon 0.9395540389522419\n",
            "episode 127, reward -161, memory_length 2000, epsilon 0.9390893781076574\n",
            "episode 128, reward -181, memory_length 2000, epsilon 0.9386249495354222\n",
            "episode 129, reward -314, memory_length 2000, epsilon 0.9381607531194293\n",
            "episode 130, reward -649, memory_length 2000, epsilon 0.9376967887436294\n",
            "episode 131, reward 17, memory_length 2000, epsilon 0.9372330562920316\n",
            "episode 132, reward -276, memory_length 2000, epsilon 0.9367695556487027\n",
            "episode 133, reward -273, memory_length 2000, epsilon 0.9363062866977675\n",
            "episode 134, reward -210, memory_length 2000, epsilon 0.9358432493234088\n",
            "episode 135, reward 3, memory_length 2000, epsilon 0.9353804434098673\n",
            "episode 136, reward -311, memory_length 2000, epsilon 0.9349178688414413\n",
            "episode 137, reward -258, memory_length 2000, epsilon 0.9344555255024876\n",
            "episode 138, reward 149, memory_length 2000, epsilon 0.9339934132774199\n",
            "episode 139, reward -283, memory_length 2000, epsilon 0.9335315320507104\n",
            "episode 140, reward -25, memory_length 2000, epsilon 0.9330698817068888\n",
            "episode 141, reward -82, memory_length 2000, epsilon 0.9326084621305423\n",
            "episode 142, reward -531, memory_length 2000, epsilon 0.9321472732063162\n",
            "episode 143, reward -99, memory_length 2000, epsilon 0.9316863148189132\n",
            "episode 144, reward -349, memory_length 2000, epsilon 0.9312255868530936\n",
            "episode 145, reward -410, memory_length 2000, epsilon 0.9307650891936757\n",
            "episode 146, reward -77, memory_length 2000, epsilon 0.9303048217255349\n",
            "episode 147, reward -75, memory_length 2000, epsilon 0.9298447843336041\n",
            "episode 148, reward -50, memory_length 2000, epsilon 0.9293849769028744\n",
            "episode 149, reward -42, memory_length 2000, epsilon 0.9289253993183936\n",
            "episode 150, reward -244, memory_length 2000, epsilon 0.9284660514652673\n",
            "episode 151, reward 166, memory_length 2000, epsilon 0.9280069332286588\n",
            "episode 152, reward -55, memory_length 2000, epsilon 0.9275480444937885\n",
            "episode 153, reward -134, memory_length 2000, epsilon 0.927089385145934\n",
            "episode 154, reward -166, memory_length 2000, epsilon 0.9266309550704304\n",
            "episode 155, reward -80, memory_length 2000, epsilon 0.9261727541526704\n",
            "episode 156, reward 102, memory_length 2000, epsilon 0.9257147822781039\n",
            "episode 157, reward -97, memory_length 2000, epsilon 0.9252570393322376\n",
            "episode 158, reward -201, memory_length 2000, epsilon 0.9247995252006359\n",
            "episode 159, reward 76, memory_length 2000, epsilon 0.9243422397689204\n",
            "episode 160, reward -561, memory_length 2000, epsilon 0.9238851829227694\n",
            "episode 161, reward -341, memory_length 2000, epsilon 0.923428354547919\n",
            "episode 162, reward -78, memory_length 2000, epsilon 0.922971754530162\n",
            "episode 163, reward -60, memory_length 2000, epsilon 0.9225153827553484\n",
            "episode 164, reward 122, memory_length 2000, epsilon 0.9220592391093853\n",
            "episode 165, reward -154, memory_length 2000, epsilon 0.9216033234782365\n",
            "episode 166, reward -415, memory_length 2000, epsilon 0.9211476357479235\n",
            "episode 167, reward -185, memory_length 2000, epsilon 0.9206921758045241\n",
            "episode 168, reward -146, memory_length 2000, epsilon 0.9202369435341734\n",
            "episode 169, reward -46, memory_length 2000, epsilon 0.9197819388230634\n",
            "episode 170, reward -68, memory_length 2000, epsilon 0.9193271615574428\n",
            "episode 171, reward -71, memory_length 2000, epsilon 0.9188726116236172\n",
            "episode 172, reward -25, memory_length 2000, epsilon 0.9184182889079494\n",
            "episode 173, reward -145, memory_length 2000, epsilon 0.9179641932968585\n",
            "episode 174, reward -42, memory_length 2000, epsilon 0.9175103246768207\n",
            "episode 175, reward -213, memory_length 2000, epsilon 0.9170566829343688\n",
            "episode 176, reward -52, memory_length 2000, epsilon 0.9166032679560924\n",
            "episode 177, reward 30, memory_length 2000, epsilon 0.9161500796286376\n",
            "episode 178, reward -149, memory_length 2000, epsilon 0.9156971178387074\n",
            "episode 179, reward 206, memory_length 2000, epsilon 0.9152443824730615\n",
            "episode 180, reward -57, memory_length 2000, epsilon 0.9147918734185159\n",
            "episode 181, reward 35, memory_length 2000, epsilon 0.9143395905619434\n",
            "episode 182, reward -174, memory_length 2000, epsilon 0.9138875337902731\n",
            "episode 183, reward -209, memory_length 2000, epsilon 0.913435702990491\n",
            "episode 184, reward -492, memory_length 2000, epsilon 0.9129840980496394\n",
            "episode 185, reward -224, memory_length 2000, epsilon 0.9125327188548171\n",
            "episode 186, reward -7, memory_length 2000, epsilon 0.9120815652931792\n",
            "episode 187, reward 37, memory_length 2000, epsilon 0.9116306372519372\n",
            "episode 188, reward 129, memory_length 2000, epsilon 0.9111799346183593\n",
            "episode 189, reward 56, memory_length 2000, epsilon 0.9107294572797696\n",
            "episode 190, reward -113, memory_length 2000, epsilon 0.9102792051235491\n",
            "episode 191, reward -47, memory_length 2000, epsilon 0.9098291780371345\n",
            "episode 192, reward -366, memory_length 2000, epsilon 0.9093793759080191\n",
            "episode 193, reward 57, memory_length 2000, epsilon 0.9089297986237523\n",
            "episode 194, reward 12, memory_length 2000, epsilon 0.90848044607194\n",
            "episode 195, reward -204, memory_length 2000, epsilon 0.9080313181402437\n",
            "episode 196, reward -185, memory_length 2000, epsilon 0.9075824147163817\n",
            "episode 197, reward 263, memory_length 2000, epsilon 0.907133735688128\n",
            "episode 198, reward -121, memory_length 2000, epsilon 0.906685280943313\n",
            "episode 199, reward -24, memory_length 2000, epsilon 0.9062370503698229\n",
            "episode 200, reward 28, memory_length 2000, epsilon 0.9057890438555999\n",
            "episode 201, reward 245, memory_length 2000, epsilon 0.9053412612886427\n",
            "episode 202, reward 60, memory_length 2000, epsilon 0.9048937025570055\n",
            "episode 203, reward -118, memory_length 2000, epsilon 0.9044463675487985\n",
            "episode 204, reward -114, memory_length 2000, epsilon 0.903999256152188\n",
            "episode 205, reward -92, memory_length 2000, epsilon 0.9035523682553963\n",
            "episode 206, reward 208, memory_length 2000, epsilon 0.9031057037467013\n",
            "episode 207, reward -247, memory_length 2000, epsilon 0.9026592625144368\n",
            "episode 208, reward -5, memory_length 2000, epsilon 0.9022130444469927\n",
            "episode 209, reward -21, memory_length 2000, epsilon 0.9017670494328143\n",
            "episode 210, reward -302, memory_length 2000, epsilon 0.9013212773604029\n",
            "episode 211, reward -70, memory_length 2000, epsilon 0.9008757281183155\n",
            "episode 212, reward 32, memory_length 2000, epsilon 0.9004304015951649\n",
            "episode 213, reward -156, memory_length 2000, epsilon 0.8999852976796191\n",
            "episode 214, reward 220, memory_length 2000, epsilon 0.8995404162604025\n",
            "episode 215, reward 40, memory_length 2000, epsilon 0.8990957572262945\n",
            "episode 216, reward -38, memory_length 2000, epsilon 0.8986513204661305\n",
            "episode 217, reward 134, memory_length 2000, epsilon 0.8982071058688013\n",
            "episode 218, reward 191, memory_length 2000, epsilon 0.8977631133232531\n",
            "episode 219, reward -42, memory_length 2000, epsilon 0.8973193427184879\n",
            "episode 220, reward 144, memory_length 2000, epsilon 0.896875793943563\n",
            "episode 221, reward -70, memory_length 2000, epsilon 0.8964324668875912\n",
            "episode 222, reward 132, memory_length 2000, epsilon 0.8959893614397407\n",
            "episode 223, reward -115, memory_length 2000, epsilon 0.8955464774892351\n",
            "episode 224, reward -164, memory_length 2000, epsilon 0.8951038149253536\n",
            "episode 225, reward -74, memory_length 2000, epsilon 0.8946613736374305\n",
            "episode 226, reward 44, memory_length 2000, epsilon 0.8942191535148554\n",
            "episode 227, reward -115, memory_length 2000, epsilon 0.8937771544470732\n",
            "episode 228, reward 35, memory_length 2000, epsilon 0.8933353763235842\n",
            "episode 229, reward -183, memory_length 2000, epsilon 0.8928938190339439\n",
            "episode 230, reward 145, memory_length 2000, epsilon 0.892452482467763\n",
            "episode 231, reward 129, memory_length 2000, epsilon 0.8920113665147074\n",
            "episode 232, reward 44, memory_length 2000, epsilon 0.8915704710644979\n",
            "episode 233, reward -129, memory_length 2000, epsilon 0.8911297960069108\n",
            "episode 234, reward 49, memory_length 2000, epsilon 0.8906893412317772\n",
            "episode 235, reward 24, memory_length 2000, epsilon 0.8902491066289836\n",
            "episode 236, reward -352, memory_length 2000, epsilon 0.8898090920884711\n",
            "episode 237, reward 0, memory_length 2000, epsilon 0.8893692975002364\n",
            "episode 238, reward 49, memory_length 2000, epsilon 0.8889297227543306\n",
            "episode 239, reward -239, memory_length 2000, epsilon 0.88849036774086\n",
            "episode 240, reward -131, memory_length 2000, epsilon 0.888051232349986\n",
            "episode 241, reward 49, memory_length 2000, epsilon 0.8876123164719245\n",
            "episode 242, reward -94, memory_length 2000, epsilon 0.8871736199969468\n",
            "episode 243, reward -22, memory_length 2000, epsilon 0.8867351428153787\n",
            "episode 244, reward -176, memory_length 2000, epsilon 0.8862968848176008\n",
            "episode 245, reward -56, memory_length 2000, epsilon 0.8858588458940487\n",
            "episode 246, reward -80, memory_length 2000, epsilon 0.8854210259352127\n",
            "episode 247, reward -244, memory_length 2000, epsilon 0.8849834248316377\n",
            "episode 248, reward -23, memory_length 2000, epsilon 0.8845460424739234\n",
            "episode 249, reward 50, memory_length 2000, epsilon 0.8841088787527244\n",
            "episode 250, reward -34, memory_length 2000, epsilon 0.8836719335587495\n",
            "episode 251, reward 103, memory_length 2000, epsilon 0.8832352067827626\n",
            "episode 252, reward 53, memory_length 2000, epsilon 0.8827986983155819\n",
            "episode 253, reward -59, memory_length 2000, epsilon 0.8823624080480803\n",
            "episode 254, reward -122, memory_length 2000, epsilon 0.8819263358711854\n",
            "episode 255, reward -265, memory_length 2000, epsilon 0.881490481675879\n",
            "episode 256, reward -213, memory_length 2000, epsilon 0.8810548453531973\n",
            "episode 257, reward -15, memory_length 2000, epsilon 0.8806194267942318\n",
            "episode 258, reward -86, memory_length 2000, epsilon 0.8801842258901273\n",
            "episode 259, reward -126, memory_length 2000, epsilon 0.879749242532084\n",
            "episode 260, reward -326, memory_length 2000, epsilon 0.8793144766113556\n",
            "episode 261, reward -13, memory_length 2000, epsilon 0.8788799280192511\n",
            "episode 262, reward 67, memory_length 2000, epsilon 0.8784455966471331\n",
            "episode 263, reward 4, memory_length 2000, epsilon 0.8780114823864188\n",
            "episode 264, reward 66, memory_length 2000, epsilon 0.8775775851285795\n",
            "episode 265, reward 40, memory_length 2000, epsilon 0.8771439047651411\n",
            "episode 266, reward -22, memory_length 2000, epsilon 0.8767104411876834\n",
            "episode 267, reward 489, memory_length 2000, epsilon 0.8762771942878405\n",
            "episode 268, reward -129, memory_length 2000, epsilon 0.8758441639573007\n",
            "episode 269, reward -311, memory_length 2000, epsilon 0.8754113500878064\n",
            "episode 270, reward -2, memory_length 2000, epsilon 0.8749787525711541\n",
            "episode 271, reward 17, memory_length 2000, epsilon 0.8745463712991944\n",
            "episode 272, reward -75, memory_length 2000, epsilon 0.8741142061638321\n",
            "episode 273, reward 105, memory_length 2000, epsilon 0.8736822570570258\n",
            "episode 274, reward 42, memory_length 2000, epsilon 0.8732505238707883\n",
            "episode 275, reward -70, memory_length 2000, epsilon 0.8728190064971862\n",
            "episode 276, reward 501, memory_length 2000, epsilon 0.8723877048283404\n",
            "episode 277, reward -228, memory_length 2000, epsilon 0.871956618756425\n",
            "episode 278, reward 32, memory_length 2000, epsilon 0.871525748173669\n",
            "episode 279, reward -160, memory_length 2000, epsilon 0.8710950929723545\n",
            "episode 280, reward -194, memory_length 2000, epsilon 0.8706646530448178\n",
            "episode 281, reward -353, memory_length 2000, epsilon 0.8702344282834488\n",
            "episode 282, reward 100, memory_length 2000, epsilon 0.8698044185806911\n",
            "episode 283, reward 435, memory_length 2000, epsilon 0.8693746238290428\n",
            "episode 284, reward -65, memory_length 2000, epsilon 0.8689450439210549\n",
            "episode 285, reward -33, memory_length 2000, epsilon 0.8685156787493323\n",
            "episode 286, reward -259, memory_length 2000, epsilon 0.868086528206534\n",
            "episode 287, reward 114, memory_length 2000, epsilon 0.8676575921853722\n",
            "episode 288, reward -244, memory_length 2000, epsilon 0.867228870578613\n",
            "episode 289, reward 58, memory_length 2000, epsilon 0.8668003632790758\n",
            "episode 290, reward -311, memory_length 2000, epsilon 0.8663720701796339\n",
            "episode 291, reward 116, memory_length 2000, epsilon 0.865943991173214\n",
            "episode 292, reward -256, memory_length 2000, epsilon 0.8655161261527964\n",
            "episode 293, reward 317, memory_length 2000, epsilon 0.8650884750114147\n",
            "episode 294, reward 49, memory_length 2000, epsilon 0.8646610376421562\n",
            "episode 295, reward 55, memory_length 2000, epsilon 0.8642338139381617\n",
            "episode 296, reward 84, memory_length 2000, epsilon 0.8638068037926251\n",
            "episode 297, reward -44, memory_length 2000, epsilon 0.8633800070987937\n",
            "episode 298, reward -31, memory_length 2000, epsilon 0.8629534237499686\n",
            "episode 299, reward 25, memory_length 2000, epsilon 0.862527053639504\n",
            "episode 300, reward -181, memory_length 2000, epsilon 0.8621008966608072\n",
            "episode 301, reward 29, memory_length 2000, epsilon 0.861674952707339\n",
            "episode 302, reward 105, memory_length 2000, epsilon 0.8612492216726134\n",
            "episode 303, reward 176, memory_length 2000, epsilon 0.8608237034501977\n",
            "episode 304, reward -314, memory_length 2000, epsilon 0.8603983979337122\n",
            "episode 305, reward -6, memory_length 2000, epsilon 0.8599733050168307\n",
            "episode 306, reward 25, memory_length 2000, epsilon 0.8595484245932798\n",
            "episode 307, reward -4, memory_length 2000, epsilon 0.8591237565568396\n",
            "episode 308, reward 304, memory_length 2000, epsilon 0.8586993008013429\n",
            "episode 309, reward 139, memory_length 2000, epsilon 0.8582750572206758\n",
            "episode 310, reward -28, memory_length 2000, epsilon 0.8578510257087774\n",
            "episode 311, reward -140, memory_length 2000, epsilon 0.8574272061596399\n",
            "episode 312, reward -301, memory_length 2000, epsilon 0.8570035984673082\n",
            "episode 313, reward -297, memory_length 2000, epsilon 0.8565802025258807\n",
            "episode 314, reward 241, memory_length 2000, epsilon 0.8561570182295082\n",
            "episode 315, reward 44, memory_length 2000, epsilon 0.8557340454723947\n",
            "episode 316, reward -287, memory_length 2000, epsilon 0.8553112841487969\n",
            "episode 317, reward -197, memory_length 2000, epsilon 0.8548887341530246\n",
            "episode 318, reward -20, memory_length 2000, epsilon 0.8544663953794402\n",
            "episode 319, reward 49, memory_length 2000, epsilon 0.8540442677224591\n",
            "episode 320, reward -320, memory_length 2000, epsilon 0.8536223510765493\n",
            "episode 321, reward -37, memory_length 2000, epsilon 0.8532006453362315\n",
            "episode 322, reward -176, memory_length 2000, epsilon 0.8527791503960795\n",
            "episode 323, reward 127, memory_length 2000, epsilon 0.8523578661507196\n",
            "episode 324, reward -209, memory_length 2000, epsilon 0.8519367924948307\n",
            "episode 325, reward 15, memory_length 2000, epsilon 0.8515159293231441\n",
            "episode 326, reward -79, memory_length 2000, epsilon 0.8510952765304444\n",
            "episode 327, reward 292, memory_length 2000, epsilon 0.8506748340115681\n",
            "episode 328, reward -78, memory_length 2000, epsilon 0.8502546016614047\n",
            "episode 329, reward -230, memory_length 2000, epsilon 0.849834579374896\n",
            "episode 330, reward 178, memory_length 2000, epsilon 0.8494147670470367\n",
            "episode 331, reward 102, memory_length 2000, epsilon 0.8489951645728735\n",
            "episode 332, reward 60, memory_length 2000, epsilon 0.8485757718475057\n",
            "episode 333, reward -221, memory_length 2000, epsilon 0.8481565887660852\n",
            "episode 334, reward 134, memory_length 2000, epsilon 0.8477376152238164\n",
            "episode 335, reward -73, memory_length 2000, epsilon 0.8473188511159557\n",
            "episode 336, reward -50, memory_length 2000, epsilon 0.8469002963378122\n",
            "episode 337, reward 8, memory_length 2000, epsilon 0.8464819507847471\n",
            "episode 338, reward -179, memory_length 2000, epsilon 0.846063814352174\n",
            "episode 339, reward 82, memory_length 2000, epsilon 0.8456458869355589\n",
            "episode 340, reward -178, memory_length 2000, epsilon 0.8452281684304199\n",
            "episode 341, reward 8, memory_length 2000, epsilon 0.8448106587323273\n",
            "episode 342, reward 384, memory_length 2000, epsilon 0.8443933577369037\n",
            "episode 343, reward -117, memory_length 2000, epsilon 0.843976265339824\n",
            "episode 344, reward -159, memory_length 2000, epsilon 0.8435593814368149\n",
            "episode 345, reward 170, memory_length 2000, epsilon 0.8431427059236555\n",
            "episode 346, reward -111, memory_length 2000, epsilon 0.842726238696177\n",
            "episode 347, reward -176, memory_length 2000, epsilon 0.8423099796502623\n",
            "episode 348, reward -344, memory_length 2000, epsilon 0.8418939286818471\n",
            "episode 349, reward -185, memory_length 2000, epsilon 0.8414780856869183\n",
            "episode 350, reward 23, memory_length 2000, epsilon 0.8410624505615153\n",
            "episode 351, reward -39, memory_length 2000, epsilon 0.8406470232017291\n",
            "episode 352, reward -288, memory_length 2000, epsilon 0.8402318035037033\n",
            "episode 353, reward -182, memory_length 2000, epsilon 0.8398167913636325\n",
            "episode 354, reward -492, memory_length 2000, epsilon 0.8394019866777639\n",
            "episode 355, reward -51, memory_length 2000, epsilon 0.8389873893423964\n",
            "episode 356, reward 80, memory_length 2000, epsilon 0.8385729992538804\n",
            "episode 357, reward -323, memory_length 2000, epsilon 0.8381588163086185\n",
            "episode 358, reward -192, memory_length 2000, epsilon 0.8377448404030652\n",
            "episode 359, reward 145, memory_length 2000, epsilon 0.8373310714337262\n",
            "episode 360, reward -7, memory_length 2000, epsilon 0.8369175092971592\n",
            "episode 361, reward 191, memory_length 2000, epsilon 0.8365041538899741\n",
            "episode 362, reward -78, memory_length 2000, epsilon 0.8360910051088316\n",
            "episode 363, reward 31, memory_length 2000, epsilon 0.8356780628504448\n",
            "episode 364, reward -254, memory_length 2000, epsilon 0.835265327011578\n",
            "episode 365, reward 355, memory_length 2000, epsilon 0.8348527974890472\n",
            "episode 366, reward 227, memory_length 2000, epsilon 0.83444047417972\n",
            "episode 367, reward -143, memory_length 2000, epsilon 0.8340283569805158\n",
            "episode 368, reward -164, memory_length 2000, epsilon 0.833616445788405\n",
            "episode 369, reward 109, memory_length 2000, epsilon 0.83320474050041\n",
            "episode 370, reward -217, memory_length 2000, epsilon 0.8327932410136044\n",
            "episode 371, reward 157, memory_length 2000, epsilon 0.8323819472251133\n",
            "episode 372, reward 82, memory_length 2000, epsilon 0.8319708590321133\n",
            "episode 373, reward 206, memory_length 2000, epsilon 0.8315599763318324\n",
            "episode 374, reward -159, memory_length 2000, epsilon 0.8311492990215499\n",
            "episode 375, reward 220, memory_length 2000, epsilon 0.8307388269985964\n",
            "episode 376, reward 341, memory_length 2000, epsilon 0.8303285601603538\n",
            "episode 377, reward -37, memory_length 2000, epsilon 0.8299184984042556\n",
            "episode 378, reward 175, memory_length 2000, epsilon 0.8295086416277863\n",
            "episode 379, reward 52, memory_length 2000, epsilon 0.8290989897284816\n",
            "episode 380, reward -241, memory_length 2000, epsilon 0.8286895426039287\n",
            "episode 381, reward 48, memory_length 2000, epsilon 0.8282803001517657\n",
            "episode 382, reward 43, memory_length 2000, epsilon 0.8278712622696819\n",
            "episode 383, reward -185, memory_length 2000, epsilon 0.827462428855418\n",
            "episode 384, reward -188, memory_length 2000, epsilon 0.8270537998067655\n",
            "episode 385, reward 5, memory_length 2000, epsilon 0.8266453750215673\n",
            "episode 386, reward -101, memory_length 2000, epsilon 0.826237154397717\n",
            "episode 387, reward -97, memory_length 2000, epsilon 0.8258291378331598\n",
            "episode 388, reward -226, memory_length 2000, epsilon 0.8254213252258911\n",
            "episode 389, reward 94, memory_length 2000, epsilon 0.8250137164739579\n",
            "episode 390, reward -285, memory_length 2000, epsilon 0.8246063114754583\n",
            "episode 391, reward -24, memory_length 2000, epsilon 0.8241991101285405\n",
            "episode 392, reward 58, memory_length 2000, epsilon 0.8237921123314047\n",
            "episode 393, reward 246, memory_length 2000, epsilon 0.8233853179823012\n",
            "episode 394, reward -8, memory_length 2000, epsilon 0.8229787269795313\n",
            "episode 395, reward 7, memory_length 2000, epsilon 0.8225723392214475\n",
            "episode 396, reward -381, memory_length 2000, epsilon 0.8221661546064527\n",
            "episode 397, reward -158, memory_length 2000, epsilon 0.8217601730330008\n",
            "episode 398, reward -5, memory_length 2000, epsilon 0.8213543943995963\n",
            "episode 399, reward -78, memory_length 2000, epsilon 0.8209488186047948\n",
            "episode 400, reward 223, memory_length 2000, epsilon 0.820543445547202\n",
            "episode 401, reward 209, memory_length 2000, epsilon 0.8201382751254749\n",
            "episode 402, reward -240, memory_length 2000, epsilon 0.8197333072383208\n",
            "episode 403, reward -24, memory_length 2000, epsilon 0.8193285417844978\n",
            "episode 404, reward 26, memory_length 2000, epsilon 0.8189239786628142\n",
            "episode 405, reward 75, memory_length 2000, epsilon 0.8185196177721297\n",
            "episode 406, reward -244, memory_length 2000, epsilon 0.8181154590113539\n",
            "episode 407, reward -573, memory_length 2000, epsilon 0.8177115022794469\n",
            "episode 408, reward -20, memory_length 2000, epsilon 0.8173077474754198\n",
            "episode 409, reward -26, memory_length 2000, epsilon 0.8169041944983336\n",
            "episode 410, reward 89, memory_length 2000, epsilon 0.8165008432473004\n",
            "episode 411, reward 315, memory_length 2000, epsilon 0.8160976936214821\n",
            "episode 412, reward -255, memory_length 2000, epsilon 0.8156947455200915\n",
            "episode 413, reward 136, memory_length 2000, epsilon 0.8152919988423915\n",
            "episode 414, reward 107, memory_length 2000, epsilon 0.8148894534876954\n",
            "episode 415, reward -226, memory_length 2000, epsilon 0.8144871093553667\n",
            "episode 416, reward -218, memory_length 2000, epsilon 0.8140849663448195\n",
            "episode 417, reward -94, memory_length 2000, epsilon 0.8136830243555183\n",
            "episode 418, reward 147, memory_length 2000, epsilon 0.8132812832869774\n",
            "episode 419, reward 97, memory_length 2000, epsilon 0.8128797430387613\n",
            "episode 420, reward -136, memory_length 2000, epsilon 0.8124784035104852\n",
            "episode 421, reward 22, memory_length 2000, epsilon 0.8120772646018142\n",
            "episode 422, reward -102, memory_length 2000, epsilon 0.8116763262124636\n",
            "episode 423, reward -226, memory_length 2000, epsilon 0.8112755882421986\n",
            "episode 424, reward -293, memory_length 2000, epsilon 0.8108750505908348\n",
            "episode 425, reward 110, memory_length 2000, epsilon 0.8104747131582379\n",
            "episode 426, reward -145, memory_length 2000, epsilon 0.8100745758443235\n",
            "episode 427, reward -142, memory_length 2000, epsilon 0.8096746385490572\n",
            "episode 428, reward -31, memory_length 2000, epsilon 0.8092749011724547\n",
            "episode 429, reward 50, memory_length 2000, epsilon 0.8088753636145816\n",
            "episode 430, reward 232, memory_length 2000, epsilon 0.8084760257755536\n",
            "episode 431, reward 250, memory_length 2000, epsilon 0.8080768875555362\n",
            "episode 432, reward 146, memory_length 2000, epsilon 0.8076779488547449\n",
            "episode 433, reward -320, memory_length 2000, epsilon 0.8072792095734449\n",
            "episode 434, reward 74, memory_length 2000, epsilon 0.8068806696119515\n",
            "episode 435, reward -365, memory_length 2000, epsilon 0.8064823288706297\n",
            "episode 436, reward 15, memory_length 2000, epsilon 0.8060841872498941\n",
            "episode 437, reward -132, memory_length 2000, epsilon 0.8056862446502095\n",
            "episode 438, reward -30, memory_length 2000, epsilon 0.8052885009720903\n",
            "episode 439, reward 256, memory_length 2000, epsilon 0.8048909561161004\n",
            "episode 440, reward -41, memory_length 2000, epsilon 0.8044936099828537\n",
            "episode 441, reward -146, memory_length 2000, epsilon 0.8040964624730136\n",
            "episode 442, reward -156, memory_length 2000, epsilon 0.8036995134872933\n",
            "episode 443, reward 111, memory_length 2000, epsilon 0.8033027629264555\n",
            "episode 444, reward 270, memory_length 2000, epsilon 0.8029062106913125\n",
            "episode 445, reward 2, memory_length 2000, epsilon 0.8025098566827266\n",
            "episode 446, reward -47, memory_length 2000, epsilon 0.8021137008016086\n",
            "episode 447, reward -208, memory_length 2000, epsilon 0.8017177429489201\n",
            "episode 448, reward 99, memory_length 2000, epsilon 0.8013219830256715\n",
            "episode 449, reward 30, memory_length 2000, epsilon 0.8009264209329227\n",
            "episode 450, reward 80, memory_length 2000, epsilon 0.8005310565717833\n",
            "episode 451, reward 42, memory_length 2000, epsilon 0.8001358898434121\n",
            "episode 452, reward -307, memory_length 2000, epsilon 0.7997409206490175\n",
            "episode 453, reward 508, memory_length 2000, epsilon 0.7993461488898572\n",
            "episode 454, reward -191, memory_length 2000, epsilon 0.7989515744672382\n",
            "episode 455, reward 146, memory_length 2000, epsilon 0.798557197282517\n",
            "episode 456, reward 78, memory_length 2000, epsilon 0.7981630172370993\n",
            "episode 457, reward 85, memory_length 2000, epsilon 0.7977690342324398\n",
            "episode 458, reward 94, memory_length 2000, epsilon 0.797375248170043\n",
            "episode 459, reward -70, memory_length 2000, epsilon 0.7969816589514624\n",
            "episode 460, reward 154, memory_length 2000, epsilon 0.7965882664783007\n",
            "episode 461, reward 114, memory_length 2000, epsilon 0.7961950706522096\n",
            "episode 462, reward 39, memory_length 2000, epsilon 0.7958020713748902\n",
            "episode 463, reward -101, memory_length 2000, epsilon 0.7954092685480929\n",
            "episode 464, reward -246, memory_length 2000, epsilon 0.7950166620736169\n",
            "episode 465, reward 61, memory_length 2000, epsilon 0.7946242518533103\n",
            "episode 466, reward 493, memory_length 2000, epsilon 0.7942320377890709\n",
            "episode 467, reward 4, memory_length 2000, epsilon 0.7938400197828449\n",
            "episode 468, reward -24, memory_length 2000, epsilon 0.793448197736628\n",
            "episode 469, reward -12, memory_length 2000, epsilon 0.7930565715524647\n",
            "episode 470, reward -24, memory_length 2000, epsilon 0.7926651411324482\n",
            "episode 471, reward -12, memory_length 2000, epsilon 0.7922739063787212\n",
            "episode 472, reward 116, memory_length 2000, epsilon 0.7918828671934747\n",
            "episode 473, reward -105, memory_length 2000, epsilon 0.7914920234789491\n",
            "episode 474, reward 178, memory_length 2000, epsilon 0.7911013751374335\n",
            "episode 475, reward -94, memory_length 2000, epsilon 0.7907109220712659\n",
            "episode 476, reward -37, memory_length 2000, epsilon 0.7903206641828326\n",
            "episode 477, reward 202, memory_length 2000, epsilon 0.7899306013745697\n",
            "episode 478, reward 85, memory_length 2000, epsilon 0.789540733548961\n",
            "episode 479, reward -167, memory_length 2000, epsilon 0.7891510606085399\n",
            "episode 480, reward -280, memory_length 2000, epsilon 0.7887615824558879\n",
            "episode 481, reward -258, memory_length 2000, epsilon 0.7883722989936356\n",
            "episode 482, reward -213, memory_length 2000, epsilon 0.787983210124462\n",
            "episode 483, reward 106, memory_length 2000, epsilon 0.7875943157510951\n",
            "episode 484, reward -75, memory_length 2000, epsilon 0.7872056157763112\n",
            "episode 485, reward -176, memory_length 2000, epsilon 0.7868171101029353\n",
            "episode 486, reward 232, memory_length 2000, epsilon 0.7864287986338409\n",
            "episode 487, reward -195, memory_length 2000, epsilon 0.7860406812719501\n",
            "episode 488, reward -235, memory_length 2000, epsilon 0.7856527579202339\n",
            "episode 489, reward -117, memory_length 2000, epsilon 0.7852650284817111\n",
            "episode 490, reward 166, memory_length 2000, epsilon 0.7848774928594494\n",
            "episode 491, reward -274, memory_length 2000, epsilon 0.7844901509565652\n",
            "episode 492, reward 34, memory_length 2000, epsilon 0.7841030026762225\n",
            "episode 493, reward -291, memory_length 2000, epsilon 0.7837160479216346\n",
            "episode 494, reward 54, memory_length 2000, epsilon 0.7833292865960627\n",
            "episode 495, reward -175, memory_length 2000, epsilon 0.7829427186028165\n",
            "episode 496, reward -57, memory_length 2000, epsilon 0.782556343845254\n",
            "episode 497, reward -39, memory_length 2000, epsilon 0.7821701622267814\n",
            "episode 498, reward 32, memory_length 2000, epsilon 0.7817841736508534\n",
            "episode 499, reward 170, memory_length 2000, epsilon 0.7813983780209728\n",
            "episode 500, reward -231, memory_length 2000, epsilon 0.7810127752406908\n",
            "episode 501, reward 85, memory_length 2000, epsilon 0.7806273652136066\n",
            "episode 502, reward 16, memory_length 2000, epsilon 0.7802421478433677\n",
            "episode 503, reward -164, memory_length 2000, epsilon 0.7798571230336698\n",
            "episode 504, reward 99, memory_length 2000, epsilon 0.7794722906882566\n",
            "episode 505, reward -113, memory_length 2000, epsilon 0.7790876507109202\n",
            "episode 506, reward 56, memory_length 2000, epsilon 0.7787032030055004\n",
            "episode 507, reward -150, memory_length 2000, epsilon 0.7783189474758854\n",
            "episode 508, reward -93, memory_length 2000, epsilon 0.7779348840260113\n",
            "episode 509, reward 220, memory_length 2000, epsilon 0.777551012559862\n",
            "episode 510, reward 69, memory_length 2000, epsilon 0.7771673329814701\n",
            "episode 511, reward -250, memory_length 2000, epsilon 0.7767838451949154\n",
            "episode 512, reward -61, memory_length 2000, epsilon 0.7764005491043259\n",
            "episode 513, reward -147, memory_length 2000, epsilon 0.7760174446138777\n",
            "episode 514, reward 214, memory_length 2000, epsilon 0.7756345316277946\n",
            "episode 515, reward -188, memory_length 2000, epsilon 0.7752518100503484\n",
            "episode 516, reward -55, memory_length 2000, epsilon 0.7748692797858587\n",
            "episode 517, reward -335, memory_length 2000, epsilon 0.774486940738693\n",
            "episode 518, reward -120, memory_length 2000, epsilon 0.7741047928132664\n",
            "episode 519, reward 115, memory_length 2000, epsilon 0.773722835914042\n",
            "episode 520, reward 111, memory_length 2000, epsilon 0.7733410699455306\n",
            "episode 521, reward 41, memory_length 2000, epsilon 0.7729594948122906\n",
            "episode 522, reward -192, memory_length 2000, epsilon 0.7725781104189283\n",
            "episode 523, reward -101, memory_length 2000, epsilon 0.7721969166700976\n",
            "episode 524, reward -23, memory_length 2000, epsilon 0.7718159134705\n",
            "episode 525, reward 152, memory_length 2000, epsilon 0.7714351007248849\n",
            "episode 526, reward -135, memory_length 2000, epsilon 0.7710544783380486\n",
            "episode 527, reward 214, memory_length 2000, epsilon 0.7706740462148362\n",
            "episode 528, reward -152, memory_length 2000, epsilon 0.770293804260139\n",
            "episode 529, reward 334, memory_length 2000, epsilon 0.7699137523788971\n",
            "episode 530, reward -81, memory_length 2000, epsilon 0.7695338904760971\n",
            "episode 531, reward 125, memory_length 2000, epsilon 0.7691542184567738\n",
            "episode 532, reward 278, memory_length 2000, epsilon 0.768774736226009\n",
            "episode 533, reward -68, memory_length 2000, epsilon 0.7683954436889322\n",
            "episode 534, reward -48, memory_length 2000, epsilon 0.7680163407507201\n",
            "episode 535, reward 7, memory_length 2000, epsilon 0.7676374273165975\n",
            "episode 536, reward -114, memory_length 2000, epsilon 0.7672587032918353\n",
            "episode 537, reward -94, memory_length 2000, epsilon 0.7668801685817531\n",
            "episode 538, reward -98, memory_length 2000, epsilon 0.766501823091717\n",
            "episode 539, reward -294, memory_length 2000, epsilon 0.7661236667271405\n",
            "episode 540, reward 94, memory_length 2000, epsilon 0.7657456993934846\n",
            "episode 541, reward 92, memory_length 2000, epsilon 0.7653679209962576\n",
            "episode 542, reward -98, memory_length 2000, epsilon 0.7649903314410147\n",
            "episode 543, reward -154, memory_length 2000, epsilon 0.7646129306333587\n",
            "episode 544, reward 292, memory_length 2000, epsilon 0.7642357184789392\n",
            "episode 545, reward -32, memory_length 2000, epsilon 0.7638586948834531\n",
            "episode 546, reward -11, memory_length 2000, epsilon 0.7634818597526449\n",
            "episode 547, reward 148, memory_length 2000, epsilon 0.7631052129923055\n",
            "episode 548, reward -71, memory_length 2000, epsilon 0.7627287545082733\n",
            "episode 549, reward -422, memory_length 2000, epsilon 0.7623524842064335\n",
            "episode 550, reward 155, memory_length 2000, epsilon 0.7619764019927188\n",
            "episode 551, reward 3, memory_length 2000, epsilon 0.7616005077731085\n",
            "episode 552, reward 511, memory_length 2000, epsilon 0.7612248014536289\n",
            "episode 553, reward -163, memory_length 2000, epsilon 0.7608492829403538\n",
            "episode 554, reward 148, memory_length 2000, epsilon 0.7604739521394033\n",
            "episode 555, reward 117, memory_length 2000, epsilon 0.7600988089569446\n",
            "episode 556, reward -242, memory_length 2000, epsilon 0.7597238532991921\n",
            "episode 557, reward -2, memory_length 2000, epsilon 0.759349085072407\n",
            "episode 558, reward -208, memory_length 2000, epsilon 0.7589745041828969\n",
            "episode 559, reward 595, memory_length 2000, epsilon 0.7586001105370167\n",
            "episode 560, reward -244, memory_length 2000, epsilon 0.7582259040411682\n",
            "episode 561, reward 121, memory_length 2000, epsilon 0.7578518846017995\n",
            "episode 562, reward 210, memory_length 2000, epsilon 0.7574780521254059\n",
            "episode 563, reward 43, memory_length 2000, epsilon 0.7571044065185292\n",
            "episode 564, reward -39, memory_length 2000, epsilon 0.7567309476877581\n",
            "episode 565, reward -99, memory_length 2000, epsilon 0.7563576755397277\n",
            "episode 566, reward 143, memory_length 2000, epsilon 0.7559845899811201\n",
            "episode 567, reward 75, memory_length 2000, epsilon 0.7556116909186639\n",
            "episode 568, reward -39, memory_length 2000, epsilon 0.7552389782591343\n",
            "episode 569, reward -170, memory_length 2000, epsilon 0.7548664519093531\n",
            "episode 570, reward 211, memory_length 2000, epsilon 0.7544941117761887\n",
            "episode 571, reward 0, memory_length 2000, epsilon 0.7541219577665563\n",
            "episode 572, reward -193, memory_length 2000, epsilon 0.7537499897874171\n",
            "episode 573, reward 187, memory_length 2000, epsilon 0.7533782077457792\n",
            "episode 574, reward -30, memory_length 2000, epsilon 0.7530066115486973\n",
            "episode 575, reward -195, memory_length 2000, epsilon 0.752635201103272\n",
            "episode 576, reward 32, memory_length 2000, epsilon 0.7522639763166509\n",
            "episode 577, reward 359, memory_length 2000, epsilon 0.7518929370960278\n",
            "episode 578, reward 206, memory_length 2000, epsilon 0.7515220833486427\n",
            "episode 579, reward -349, memory_length 2000, epsilon 0.7511514149817824\n",
            "episode 580, reward 49, memory_length 2000, epsilon 0.7507809319027796\n",
            "episode 581, reward -29, memory_length 2000, epsilon 0.7504106340190136\n",
            "episode 582, reward -68, memory_length 2000, epsilon 0.7500405212379101\n",
            "episode 583, reward -298, memory_length 2000, epsilon 0.7496705934669406\n",
            "episode 584, reward -68, memory_length 2000, epsilon 0.7493008506136236\n",
            "episode 585, reward 108, memory_length 2000, epsilon 0.7489312925855229\n",
            "episode 586, reward -113, memory_length 2000, epsilon 0.7485619192902493\n",
            "episode 587, reward 116, memory_length 2000, epsilon 0.7481927306354593\n",
            "episode 588, reward 205, memory_length 2000, epsilon 0.7478237265288558\n",
            "episode 589, reward 132, memory_length 2000, epsilon 0.7474549068781877\n",
            "episode 590, reward -50, memory_length 2000, epsilon 0.7470862715912503\n",
            "episode 591, reward -337, memory_length 2000, epsilon 0.7467178205758846\n",
            "episode 592, reward -136, memory_length 2000, epsilon 0.7463495537399778\n",
            "episode 593, reward 363, memory_length 2000, epsilon 0.7459814709914633\n",
            "episode 594, reward -218, memory_length 2000, epsilon 0.7456135722383205\n",
            "episode 595, reward 176, memory_length 2000, epsilon 0.7452458573885745\n",
            "episode 596, reward 211, memory_length 2000, epsilon 0.7448783263502966\n",
            "episode 597, reward -140, memory_length 2000, epsilon 0.7445109790316042\n",
            "episode 598, reward 103, memory_length 2000, epsilon 0.7441438153406604\n",
            "episode 599, reward -124, memory_length 2000, epsilon 0.7437768351856743\n",
            "episode 600, reward 380, memory_length 2000, epsilon 0.7434100384749007\n",
            "episode 601, reward 107, memory_length 2000, epsilon 0.7430434251166406\n",
            "episode 602, reward 193, memory_length 2000, epsilon 0.7426769950192406\n",
            "episode 603, reward 140, memory_length 2000, epsilon 0.7423107480910931\n",
            "episode 604, reward 86, memory_length 2000, epsilon 0.7419446842406366\n",
            "episode 605, reward 99, memory_length 2000, epsilon 0.7415788033763548\n",
            "episode 606, reward -217, memory_length 2000, epsilon 0.7412131054067778\n",
            "episode 607, reward -204, memory_length 2000, epsilon 0.7408475902404807\n",
            "episode 608, reward 273, memory_length 2000, epsilon 0.7404822577860852\n",
            "episode 609, reward 30, memory_length 2000, epsilon 0.7401171079522579\n",
            "episode 610, reward 389, memory_length 2000, epsilon 0.7397521406477116\n",
            "episode 611, reward -114, memory_length 2000, epsilon 0.739387355781204\n",
            "episode 612, reward 296, memory_length 2000, epsilon 0.7390227532615391\n",
            "episode 613, reward 84, memory_length 2000, epsilon 0.7386583329975664\n",
            "episode 614, reward 107, memory_length 2000, epsilon 0.7382940948981808\n",
            "episode 615, reward 119, memory_length 2000, epsilon 0.7379300388723227\n",
            "episode 616, reward 168, memory_length 2000, epsilon 0.7375661648289781\n",
            "episode 617, reward -213, memory_length 2000, epsilon 0.7372024726771784\n",
            "episode 618, reward -122, memory_length 2000, epsilon 0.7368389623260008\n",
            "episode 619, reward -1, memory_length 2000, epsilon 0.7364756336845675\n",
            "episode 620, reward -154, memory_length 2000, epsilon 0.7361124866620463\n",
            "episode 621, reward -137, memory_length 2000, epsilon 0.7357495211676507\n",
            "episode 622, reward 128, memory_length 2000, epsilon 0.7353867371106392\n",
            "episode 623, reward -71, memory_length 2000, epsilon 0.7350241344003157\n",
            "episode 624, reward -20, memory_length 2000, epsilon 0.7346617129460296\n",
            "episode 625, reward -62, memory_length 2000, epsilon 0.7342994726571753\n",
            "episode 626, reward -93, memory_length 2000, epsilon 0.7339374134431932\n",
            "episode 627, reward -186, memory_length 2000, epsilon 0.7335755352135681\n",
            "episode 628, reward 46, memory_length 2000, epsilon 0.7332138378778307\n",
            "episode 629, reward 109, memory_length 2000, epsilon 0.7328523213455564\n",
            "episode 630, reward 166, memory_length 2000, epsilon 0.7324909855263663\n",
            "episode 631, reward 113, memory_length 2000, epsilon 0.7321298303299262\n",
            "episode 632, reward 63, memory_length 2000, epsilon 0.7317688556659475\n",
            "episode 633, reward 263, memory_length 2000, epsilon 0.7314080614441866\n",
            "episode 634, reward 65, memory_length 2000, epsilon 0.7310474475744446\n",
            "episode 635, reward 218, memory_length 2000, epsilon 0.7306870139665684\n",
            "episode 636, reward -92, memory_length 2000, epsilon 0.7303267605304495\n",
            "episode 637, reward -264, memory_length 2000, epsilon 0.7299666871760244\n",
            "episode 638, reward -131, memory_length 2000, epsilon 0.7296067938132749\n",
            "episode 639, reward 40, memory_length 2000, epsilon 0.7292470803522275\n",
            "episode 640, reward 49, memory_length 2000, epsilon 0.7288875467029541\n",
            "episode 641, reward -123, memory_length 2000, epsilon 0.7285281927755709\n",
            "episode 642, reward -37, memory_length 2000, epsilon 0.7281690184802397\n",
            "episode 643, reward -59, memory_length 2000, epsilon 0.727810023727167\n",
            "episode 644, reward -155, memory_length 2000, epsilon 0.7274512084266038\n",
            "episode 645, reward -232, memory_length 2000, epsilon 0.7270925724888465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfNw_b0F8jWo"
      },
      "source": [
        "## save stuff as pickle\n",
        "import os\n",
        "def save_pickle(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# make directory\n",
        "if not os.path.exists(\"saved_pickle_files\"):\n",
        "    os.mkdir(\"saved_pickle_files\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxFFaUhO8jWr"
      },
      "source": [
        "# save rewards_per_episode\n",
        "save_pickle(rewards_per_episode, \"saved_pickle_files/rewards_per_episode\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AVVCuQ78jWt"
      },
      "source": [
        "### Tracking Convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anY4OrFt8jWt"
      },
      "source": [
        "\n",
        "# plot results\n",
        "with open('saved_pickle_files/rewards_per_episode.pkl', 'rb') as f:\n",
        "    rewards_per_episode = pickle.load(f)\n",
        "\n",
        "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
        "plt.xlabel(\"episode number\")\n",
        "plt.ylabel(\"reward per episode\")\n",
        "\n",
        "# save plots in saved_plots/ directory\n",
        "plt.savefig('rewards.png')\n",
        "\n",
        "print(\"Average reward of last 100 episodes is {0}\".format(np.mean(rewards_per_episode[-100:]))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWpy4aAk8jWw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPzqSgF8jW0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfDqenlK8jW3"
      },
      "source": [
        "#### Epsilon-decay sample function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOnjxMl8jW3"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "Try building a similar epsilon-decay function for your model.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CHtA_uC8jW4"
      },
      "source": [
        "time = np.arange(0,10000)\n",
        "epsilon = []\n",
        "for i in range(0,10000):\n",
        "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7zz76zz8jW6"
      },
      "source": [
        "plt.plot(time, epsilon)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62JuOwk18jW-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}