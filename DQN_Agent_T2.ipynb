{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DQN_Agent_T2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemantTiwariGitHub/RLCabs/blob/main/DQN_Agent_T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CseJVoBt8jWJ"
      },
      "source": [
        "### Cab-Driver Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-nxS9L8jWK"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import deque\n",
        "import collections\n",
        "import pickle\n",
        "import time \n",
        "\n",
        "# for building DQN model\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# for plotting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the environment\n",
        "from Env import CabDriver"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPZChKPx8jWO"
      },
      "source": [
        "#### Defining Time Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fppApKKo8jWP"
      },
      "source": [
        "# Loading the time matrix provided\n",
        "Time_matrix = np.load(\"TM.npy\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DBeLhG48jWc"
      },
      "source": [
        "#Defining a function to save the Q-dictionary as a pickle file\n",
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjaZhYb28jWe"
      },
      "source": [
        "### Agent Class\n",
        "\n",
        "If you are using this framework, you need to fill the following to complete the following code block:\n",
        "1. State and Action Size\n",
        "2. Hyperparameters\n",
        "3. Create a neural-network model in function 'build_model()'\n",
        "4. Define epsilon-greedy strategy in function 'get_action()'\n",
        "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
        "6. Complete the 'train_model()' function with following logic:\n",
        "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
        "      - Initialise your input and output batch for training the model\n",
        "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
        "      - Get Q(s', a) values from the last trained model\n",
        "      - Update the input batch as your encoded state and output batch as your Q-values\n",
        "      - Then fit your DQN model using the updated input and output batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuqIwf-O8jWf"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # Define size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Write here: Specify you hyper parameters for the DQN\n",
        "        self.discount_factor = 0.95\n",
        "        self.learning_rate = 0.01       \n",
        "        self.epsilon = 1\n",
        "        self.epsilon_max = 1\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.epsilon_min = 0.01\n",
        "        \n",
        "        self.batch_size = 32        \n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=2000)\n",
        "   \n",
        "        self.states_tracked = []\n",
        "        self.track_state = np.array(env.state_encod_arch1([1,2,1])).reshape(1, self.state_size)\n",
        "        \n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        # Write your code here: Add layers to your neural nets   \n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "        # the output layer: output is of size num_actions\n",
        "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
        "        model.summary\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    def get_action(self, state, possible_actions_index, actions):\n",
        "        #print(state)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # explore: choose a random action from all possible actions\n",
        "            randAction = random.choice(possible_actions_index)\n",
        "            return randAction\n",
        "        else:\n",
        "            # choose the action with the highest q(s, a)\n",
        "            # the first index corresponds to the batch size, so\n",
        "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
        "            encodedState = env.state_encod_arch1(state)\n",
        "            batchEncodedState = np.array(encodedState).reshape(1, 36)\n",
        "            q_value = self.model.predict(batchEncodedState)\n",
        "            possibleValues = [q_value[0][i] for i in possible_actions_index]\n",
        "            return possible_actions_index[np.argmax(possibleValues)]\n",
        "\n",
        "          \n",
        "         \n",
        "   \n",
        "\n",
        "    def append_sample(self, state, action, reward, next_state,done):\n",
        "    # Write your code here:\n",
        "    # save sample <s,a,r,s'> to the replay memory\n",
        "      self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    \n",
        "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
        "    def train_model(self):\n",
        "        \n",
        "        if len(self.memory) > self.batch_size:\n",
        "            \n",
        "            # sample minibatch from memory\n",
        "            minibatch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "            # initialise two matrices - update_input and update_output\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\n",
        "            actions, rewards, done = [], [], []\n",
        "\n",
        "            # populate update_input and update_output and the lists rewards, actions, done\n",
        "            for i in range(self.batch_size):\n",
        "                state, action, reward, next_state, done_boolean = minibatch[i]\n",
        "                update_input[i] = env.state_encod_arch1(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                update_output[i] = env.state_encod_arch1(next_state)\n",
        "                done.append(done_boolean)\n",
        "\n",
        "            # predict the target q-values from states s\n",
        "            target = self.model.predict(update_input)\n",
        "\n",
        "            # target for q-network\n",
        "            target_qval = self.model.predict(update_output)\n",
        "\n",
        "            # update the target values\n",
        "            for i in range(self.batch_size):\n",
        "                if done[i]:\n",
        "                    target[i][actions[i]] = rewards[i]\n",
        "                else: # non-terminal state\n",
        "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
        "\n",
        "            # model fit\n",
        "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)\n",
        "\n",
        "    def save_tracking_states(self):\n",
        "        q_value = self.model.predict(self.track_state)\n",
        "        self.states_tracked.append(q_value[0][5])\n",
        "\n",
        "    def savePickle(self, name):\n",
        "        with open(name, 'wb') as file:  \n",
        "          pickle.dump(self.model, file,pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0GAuc0j8jWj"
      },
      "source": [
        "episodeTime = 24*30 \n",
        "Episodes = 1000\n",
        "\n",
        "env = CabDriver()\n",
        "action_space, state_space, state = env.reset()\n",
        "\n",
        "state_size = 36\n",
        "action_size = len(action_space)\n",
        "\n",
        "\n",
        "agent = DQNAgent(state_size=state_size,action_size=action_size)\n",
        "\n",
        "\n",
        "rewards_per_episode, episodes = [], []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNE467Wi8jWm"
      },
      "source": [
        "### DQN block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPsqSz7W8jWm",
        "outputId": "605bf57a-5bc4-4bfa-9213-a061ad110b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "startTime = time.time()\n",
        "scoreTracked = []\n",
        "#### simulation starts ####\n",
        "for episode in range(Episodes):\n",
        "\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    # reset at the start of each episode\n",
        "    env = CabDriver()\n",
        "    action_space, state_space, state = env.reset()\n",
        "    \n",
        "    currentEpisodeTime=0\n",
        "    maxEpisodeTime = episodeTime \n",
        "   \n",
        "\n",
        "\n",
        "    while not done:\n",
        "        # get action for the current state and take a step in the environment\n",
        "        possible_actions_index, actions = env.requests(state)\n",
        "      \n",
        "        #print(state)\n",
        "        #print(actions)\n",
        "        #print(possible_actions_index)\n",
        "        \n",
        "\n",
        "        action = agent.get_action(state, possible_actions_index, actions)\n",
        "\n",
        "        #print(action)\n",
        "\n",
        "\n",
        "        next_state, reward, timePassed = env.next_state_func(state, env.action_space[action], Time_matrix)\n",
        "        currentEpisodeTime = currentEpisodeTime + timePassed\n",
        "\n",
        "        if (currentEpisodeTime > maxEpisodeTime):\n",
        "          done = True\n",
        "  \n",
        "        # save the sample <s, a, r, s', done> to the replay memory\n",
        "        agent.append_sample(state, action, reward, next_state, done)\n",
        "      \n",
        "\n",
        "        # train after each step\n",
        "        agent.train_model()\n",
        "\n",
        "        # add reward to the total score of this episode\n",
        "        score += reward\n",
        "\n",
        " \n",
        "        #print(i,\": \" , state , \" :  \" , action , \" :  \" , reward , \" : \" , next_state , \" : \" , done, \" : \" , score)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "    # store total reward obtained in this episode\n",
        "    rewards_per_episode.append(score)\n",
        "    episodes.append(episode)\n",
        "\n",
        "    # epsilon decay\n",
        "    if agent.epsilon > agent.epsilon_min:\n",
        "          agent.epsilon = agent.epsilon_min + (agent.epsilon_max - agent.epsilon_min) * np.exp(-0.0005 * episode)\n",
        "\n",
        "    # every episode:\n",
        "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}\".format(episode,\n",
        "                                                                         score,\n",
        "                                                                         len(agent.memory),\n",
        "                                                                         agent.epsilon))\n",
        "    \n",
        "    if (((episode +1) % 10) == 0):\n",
        "       agent.save_tracking_states()\n",
        "    \n",
        "    if (((episode +1 )% 500) == 0):\n",
        "      agent.save(name=\"model_weights.h5\")\n",
        "\n",
        "#### simulation complete ####\n",
        "        "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0, reward -129, memory_length 160, epsilon 1.0\n",
            "episode 1, reward 116, memory_length 292, epsilon 0.9995051237293776\n",
            "episode 2, reward -344, memory_length 425, epsilon 0.9990104948350412\n",
            "episode 3, reward -205, memory_length 571, epsilon 0.9985161131933338\n",
            "episode 4, reward -150, memory_length 713, epsilon 0.9980219786806598\n",
            "episode 5, reward -448, memory_length 856, epsilon 0.9975280911734855\n",
            "episode 6, reward -138, memory_length 997, epsilon 0.9970344505483393\n",
            "episode 7, reward -160, memory_length 1135, epsilon 0.9965410566818107\n",
            "episode 8, reward -331, memory_length 1285, epsilon 0.9960479094505515\n",
            "episode 9, reward -194, memory_length 1433, epsilon 0.9955550087312749\n",
            "episode 10, reward -177, memory_length 1568, epsilon 0.9950623544007555\n",
            "episode 11, reward -61, memory_length 1721, epsilon 0.9945699463358298\n",
            "episode 12, reward 22, memory_length 1846, epsilon 0.9940777844133959\n",
            "episode 13, reward -65, memory_length 1984, epsilon 0.9935858685104132\n",
            "episode 14, reward -287, memory_length 2000, epsilon 0.9930941985039028\n",
            "episode 15, reward -147, memory_length 2000, epsilon 0.992602774270947\n",
            "episode 16, reward -259, memory_length 2000, epsilon 0.99211159568869\n",
            "episode 17, reward -298, memory_length 2000, epsilon 0.991620662634337\n",
            "episode 18, reward -104, memory_length 2000, epsilon 0.9911299749851548\n",
            "episode 19, reward -330, memory_length 2000, epsilon 0.9906395326184715\n",
            "episode 20, reward -347, memory_length 2000, epsilon 0.9901493354116764\n",
            "episode 21, reward -368, memory_length 2000, epsilon 0.9896593832422202\n",
            "episode 22, reward 192, memory_length 2000, epsilon 0.9891696759876151\n",
            "episode 23, reward 7, memory_length 2000, epsilon 0.9886802135254339\n",
            "episode 24, reward -133, memory_length 2000, epsilon 0.9881909957333113\n",
            "episode 25, reward -15, memory_length 2000, epsilon 0.9877020224889427\n",
            "episode 26, reward -257, memory_length 2000, epsilon 0.9872132936700847\n",
            "episode 27, reward -57, memory_length 2000, epsilon 0.9867248091545553\n",
            "episode 28, reward 43, memory_length 2000, epsilon 0.9862365688202333\n",
            "episode 29, reward 157, memory_length 2000, epsilon 0.9857485725450585\n",
            "episode 30, reward -105, memory_length 2000, epsilon 0.985260820207032\n",
            "episode 31, reward -57, memory_length 2000, epsilon 0.9847733116842156\n",
            "episode 32, reward -217, memory_length 2000, epsilon 0.9842860468547323\n",
            "episode 33, reward -454, memory_length 2000, epsilon 0.9837990255967657\n",
            "episode 34, reward -120, memory_length 2000, epsilon 0.9833122477885605\n",
            "episode 35, reward -146, memory_length 2000, epsilon 0.9828257133084225\n",
            "episode 36, reward -341, memory_length 2000, epsilon 0.9823394220347178\n",
            "episode 37, reward -179, memory_length 2000, epsilon 0.9818533738458736\n",
            "episode 38, reward -79, memory_length 2000, epsilon 0.9813675686203779\n",
            "episode 39, reward -128, memory_length 2000, epsilon 0.9808820062367796\n",
            "episode 40, reward -389, memory_length 2000, epsilon 0.9803966865736877\n",
            "episode 41, reward -147, memory_length 2000, epsilon 0.9799116095097726\n",
            "episode 42, reward -127, memory_length 2000, epsilon 0.979426774923765\n",
            "episode 43, reward -397, memory_length 2000, epsilon 0.9789421826944561\n",
            "episode 44, reward -302, memory_length 2000, epsilon 0.978457832700698\n",
            "episode 45, reward -429, memory_length 2000, epsilon 0.977973724821403\n",
            "episode 46, reward 218, memory_length 2000, epsilon 0.9774898589355443\n",
            "episode 47, reward -155, memory_length 2000, epsilon 0.9770062349221554\n",
            "episode 48, reward -294, memory_length 2000, epsilon 0.9765228526603302\n",
            "episode 49, reward -176, memory_length 2000, epsilon 0.9760397120292232\n",
            "episode 50, reward -231, memory_length 2000, epsilon 0.9755568129080493\n",
            "episode 51, reward -55, memory_length 2000, epsilon 0.9750741551760836\n",
            "episode 52, reward -326, memory_length 2000, epsilon 0.9745917387126619\n",
            "episode 53, reward -37, memory_length 2000, epsilon 0.9741095633971797\n",
            "episode 54, reward -123, memory_length 2000, epsilon 0.9736276291090934\n",
            "episode 55, reward -281, memory_length 2000, epsilon 0.9731459357279194\n",
            "episode 56, reward 25, memory_length 2000, epsilon 0.9726644831332344\n",
            "episode 57, reward -415, memory_length 2000, epsilon 0.9721832712046753\n",
            "episode 58, reward -293, memory_length 2000, epsilon 0.9717022998219388\n",
            "episode 59, reward -204, memory_length 2000, epsilon 0.9712215688647823\n",
            "episode 60, reward 71, memory_length 2000, epsilon 0.970741078213023\n",
            "episode 61, reward -60, memory_length 2000, epsilon 0.9702608277465384\n",
            "episode 62, reward -420, memory_length 2000, epsilon 0.9697808173452657\n",
            "episode 63, reward -109, memory_length 2000, epsilon 0.9693010468892023\n",
            "episode 64, reward 42, memory_length 2000, epsilon 0.9688215162584056\n",
            "episode 65, reward 14, memory_length 2000, epsilon 0.968342225332993\n",
            "episode 66, reward -174, memory_length 2000, epsilon 0.9678631739931417\n",
            "episode 67, reward -291, memory_length 2000, epsilon 0.9673843621190888\n",
            "episode 68, reward -144, memory_length 2000, epsilon 0.9669057895911316\n",
            "episode 69, reward -113, memory_length 2000, epsilon 0.9664274562896267\n",
            "episode 70, reward -140, memory_length 2000, epsilon 0.9659493620949908\n",
            "episode 71, reward -398, memory_length 2000, epsilon 0.9654715068877004\n",
            "episode 72, reward 30, memory_length 2000, epsilon 0.9649938905482919\n",
            "episode 73, reward -239, memory_length 2000, epsilon 0.9645165129573607\n",
            "episode 74, reward -282, memory_length 2000, epsilon 0.9640393739955629\n",
            "episode 75, reward 25, memory_length 2000, epsilon 0.9635624735436136\n",
            "episode 76, reward -363, memory_length 2000, epsilon 0.9630858114822876\n",
            "episode 77, reward -114, memory_length 2000, epsilon 0.9626093876924194\n",
            "episode 78, reward -320, memory_length 2000, epsilon 0.962133202054903\n",
            "episode 79, reward -311, memory_length 2000, epsilon 0.9616572544506923\n",
            "episode 80, reward -268, memory_length 2000, epsilon 0.9611815447608\n",
            "episode 81, reward -182, memory_length 2000, epsilon 0.9607060728662988\n",
            "episode 82, reward -420, memory_length 2000, epsilon 0.9602308386483209\n",
            "episode 83, reward -78, memory_length 2000, epsilon 0.9597558419880575\n",
            "episode 84, reward -19, memory_length 2000, epsilon 0.9592810827667597\n",
            "episode 85, reward -97, memory_length 2000, epsilon 0.9588065608657375\n",
            "episode 86, reward -340, memory_length 2000, epsilon 0.9583322761663603\n",
            "episode 87, reward -146, memory_length 2000, epsilon 0.9578582285500573\n",
            "episode 88, reward 196, memory_length 2000, epsilon 0.9573844178983162\n",
            "episode 89, reward -328, memory_length 2000, epsilon 0.9569108440926846\n",
            "episode 90, reward -271, memory_length 2000, epsilon 0.9564375070147689\n",
            "episode 91, reward -354, memory_length 2000, epsilon 0.9559644065462349\n",
            "episode 92, reward 176, memory_length 2000, epsilon 0.9554915425688075\n",
            "episode 93, reward -293, memory_length 2000, epsilon 0.9550189149642706\n",
            "episode 94, reward -205, memory_length 2000, epsilon 0.9545465236144673\n",
            "episode 95, reward -121, memory_length 2000, epsilon 0.9540743684013001\n",
            "episode 96, reward -187, memory_length 2000, epsilon 0.9536024492067297\n",
            "episode 97, reward 20, memory_length 2000, epsilon 0.9531307659127765\n",
            "episode 98, reward -192, memory_length 2000, epsilon 0.9526593184015199\n",
            "episode 99, reward 94, memory_length 2000, epsilon 0.9521881065550976\n",
            "episode 100, reward -199, memory_length 2000, epsilon 0.9517171302557069\n",
            "episode 101, reward -186, memory_length 2000, epsilon 0.9512463893856036\n",
            "episode 102, reward 13, memory_length 2000, epsilon 0.9507758838271028\n",
            "episode 103, reward -202, memory_length 2000, epsilon 0.9503056134625776\n",
            "episode 104, reward -92, memory_length 2000, epsilon 0.9498355781744606\n",
            "episode 105, reward -125, memory_length 2000, epsilon 0.9493657778452432\n",
            "episode 106, reward -543, memory_length 2000, epsilon 0.9488962123574752\n",
            "episode 107, reward -85, memory_length 2000, epsilon 0.9484268815937651\n",
            "episode 108, reward 84, memory_length 2000, epsilon 0.9479577854367803\n",
            "episode 109, reward -456, memory_length 2000, epsilon 0.9474889237692468\n",
            "episode 110, reward -129, memory_length 2000, epsilon 0.947020296473949\n",
            "episode 111, reward 156, memory_length 2000, epsilon 0.9465519034337303\n",
            "episode 112, reward -76, memory_length 2000, epsilon 0.9460837445314924\n",
            "episode 113, reward 154, memory_length 2000, epsilon 0.9456158196501953\n",
            "episode 114, reward -185, memory_length 2000, epsilon 0.9451481286728581\n",
            "episode 115, reward -141, memory_length 2000, epsilon 0.944680671482558\n",
            "episode 116, reward -108, memory_length 2000, epsilon 0.9442134479624306\n",
            "episode 117, reward -132, memory_length 2000, epsilon 0.9437464579956699\n",
            "episode 118, reward 42, memory_length 2000, epsilon 0.9432797014655288\n",
            "episode 119, reward -321, memory_length 2000, epsilon 0.9428131782553177\n",
            "episode 120, reward 80, memory_length 2000, epsilon 0.9423468882484062\n",
            "episode 121, reward -200, memory_length 2000, epsilon 0.9418808313282216\n",
            "episode 122, reward -122, memory_length 2000, epsilon 0.9414150073782496\n",
            "episode 123, reward -80, memory_length 2000, epsilon 0.9409494162820343\n",
            "episode 124, reward -32, memory_length 2000, epsilon 0.940484057923178\n",
            "episode 125, reward -307, memory_length 2000, epsilon 0.940018932185341\n",
            "episode 126, reward 71, memory_length 2000, epsilon 0.9395540389522419\n",
            "episode 127, reward -161, memory_length 2000, epsilon 0.9390893781076574\n",
            "episode 128, reward -181, memory_length 2000, epsilon 0.9386249495354222\n",
            "episode 129, reward -314, memory_length 2000, epsilon 0.9381607531194293\n",
            "episode 130, reward -649, memory_length 2000, epsilon 0.9376967887436294\n",
            "episode 131, reward 17, memory_length 2000, epsilon 0.9372330562920316\n",
            "episode 132, reward -276, memory_length 2000, epsilon 0.9367695556487027\n",
            "episode 133, reward -273, memory_length 2000, epsilon 0.9363062866977675\n",
            "episode 134, reward -210, memory_length 2000, epsilon 0.9358432493234088\n",
            "episode 135, reward 3, memory_length 2000, epsilon 0.9353804434098673\n",
            "episode 136, reward -311, memory_length 2000, epsilon 0.9349178688414413\n",
            "episode 137, reward -258, memory_length 2000, epsilon 0.9344555255024876\n",
            "episode 138, reward 149, memory_length 2000, epsilon 0.9339934132774199\n",
            "episode 139, reward -283, memory_length 2000, epsilon 0.9335315320507104\n",
            "episode 140, reward -25, memory_length 2000, epsilon 0.9330698817068888\n",
            "episode 141, reward -82, memory_length 2000, epsilon 0.9326084621305423\n",
            "episode 142, reward -531, memory_length 2000, epsilon 0.9321472732063162\n",
            "episode 143, reward -99, memory_length 2000, epsilon 0.9316863148189132\n",
            "episode 144, reward -349, memory_length 2000, epsilon 0.9312255868530936\n",
            "episode 145, reward -410, memory_length 2000, epsilon 0.9307650891936757\n",
            "episode 146, reward -77, memory_length 2000, epsilon 0.9303048217255349\n",
            "episode 147, reward -75, memory_length 2000, epsilon 0.9298447843336041\n",
            "episode 148, reward -50, memory_length 2000, epsilon 0.9293849769028744\n",
            "episode 149, reward -42, memory_length 2000, epsilon 0.9289253993183936\n",
            "episode 150, reward -244, memory_length 2000, epsilon 0.9284660514652673\n",
            "episode 151, reward 166, memory_length 2000, epsilon 0.9280069332286588\n",
            "episode 152, reward -55, memory_length 2000, epsilon 0.9275480444937885\n",
            "episode 153, reward -134, memory_length 2000, epsilon 0.927089385145934\n",
            "episode 154, reward -166, memory_length 2000, epsilon 0.9266309550704304\n",
            "episode 155, reward -80, memory_length 2000, epsilon 0.9261727541526704\n",
            "episode 156, reward 102, memory_length 2000, epsilon 0.9257147822781039\n",
            "episode 157, reward -97, memory_length 2000, epsilon 0.9252570393322376\n",
            "episode 158, reward -201, memory_length 2000, epsilon 0.9247995252006359\n",
            "episode 159, reward 76, memory_length 2000, epsilon 0.9243422397689204\n",
            "episode 160, reward -561, memory_length 2000, epsilon 0.9238851829227694\n",
            "episode 161, reward -341, memory_length 2000, epsilon 0.923428354547919\n",
            "episode 162, reward -78, memory_length 2000, epsilon 0.922971754530162\n",
            "episode 163, reward -60, memory_length 2000, epsilon 0.9225153827553484\n",
            "episode 164, reward 122, memory_length 2000, epsilon 0.9220592391093853\n",
            "episode 165, reward -154, memory_length 2000, epsilon 0.9216033234782365\n",
            "episode 166, reward -415, memory_length 2000, epsilon 0.9211476357479235\n",
            "episode 167, reward -185, memory_length 2000, epsilon 0.9206921758045241\n",
            "episode 168, reward -146, memory_length 2000, epsilon 0.9202369435341734\n",
            "episode 169, reward -46, memory_length 2000, epsilon 0.9197819388230634\n",
            "episode 170, reward -68, memory_length 2000, epsilon 0.9193271615574428\n",
            "episode 171, reward -71, memory_length 2000, epsilon 0.9188726116236172\n",
            "episode 172, reward -25, memory_length 2000, epsilon 0.9184182889079494\n",
            "episode 173, reward -145, memory_length 2000, epsilon 0.9179641932968585\n",
            "episode 174, reward -42, memory_length 2000, epsilon 0.9175103246768207\n",
            "episode 175, reward -213, memory_length 2000, epsilon 0.9170566829343688\n",
            "episode 176, reward -52, memory_length 2000, epsilon 0.9166032679560924\n",
            "episode 177, reward 30, memory_length 2000, epsilon 0.9161500796286376\n",
            "episode 178, reward -149, memory_length 2000, epsilon 0.9156971178387074\n",
            "episode 179, reward 206, memory_length 2000, epsilon 0.9152443824730615\n",
            "episode 180, reward -57, memory_length 2000, epsilon 0.9147918734185159\n",
            "episode 181, reward 35, memory_length 2000, epsilon 0.9143395905619434\n",
            "episode 182, reward -174, memory_length 2000, epsilon 0.9138875337902731\n",
            "episode 183, reward -209, memory_length 2000, epsilon 0.913435702990491\n",
            "episode 184, reward -492, memory_length 2000, epsilon 0.9129840980496394\n",
            "episode 185, reward -224, memory_length 2000, epsilon 0.9125327188548171\n",
            "episode 186, reward -7, memory_length 2000, epsilon 0.9120815652931792\n",
            "episode 187, reward 37, memory_length 2000, epsilon 0.9116306372519372\n",
            "episode 188, reward 129, memory_length 2000, epsilon 0.9111799346183593\n",
            "episode 189, reward 56, memory_length 2000, epsilon 0.9107294572797696\n",
            "episode 190, reward -113, memory_length 2000, epsilon 0.9102792051235491\n",
            "episode 191, reward -47, memory_length 2000, epsilon 0.9098291780371345\n",
            "episode 192, reward -366, memory_length 2000, epsilon 0.9093793759080191\n",
            "episode 193, reward 57, memory_length 2000, epsilon 0.9089297986237523\n",
            "episode 194, reward 12, memory_length 2000, epsilon 0.90848044607194\n",
            "episode 195, reward -204, memory_length 2000, epsilon 0.9080313181402437\n",
            "episode 196, reward -185, memory_length 2000, epsilon 0.9075824147163817\n",
            "episode 197, reward 263, memory_length 2000, epsilon 0.907133735688128\n",
            "episode 198, reward -121, memory_length 2000, epsilon 0.906685280943313\n",
            "episode 199, reward -24, memory_length 2000, epsilon 0.9062370503698229\n",
            "episode 200, reward 28, memory_length 2000, epsilon 0.9057890438555999\n",
            "episode 201, reward 245, memory_length 2000, epsilon 0.9053412612886427\n",
            "episode 202, reward 60, memory_length 2000, epsilon 0.9048937025570055\n",
            "episode 203, reward -118, memory_length 2000, epsilon 0.9044463675487985\n",
            "episode 204, reward -114, memory_length 2000, epsilon 0.903999256152188\n",
            "episode 205, reward -92, memory_length 2000, epsilon 0.9035523682553963\n",
            "episode 206, reward 208, memory_length 2000, epsilon 0.9031057037467013\n",
            "episode 207, reward -247, memory_length 2000, epsilon 0.9026592625144368\n",
            "episode 208, reward -5, memory_length 2000, epsilon 0.9022130444469927\n",
            "episode 209, reward -21, memory_length 2000, epsilon 0.9017670494328143\n",
            "episode 210, reward -302, memory_length 2000, epsilon 0.9013212773604029\n",
            "episode 211, reward -70, memory_length 2000, epsilon 0.9008757281183155\n",
            "episode 212, reward 32, memory_length 2000, epsilon 0.9004304015951649\n",
            "episode 213, reward -156, memory_length 2000, epsilon 0.8999852976796191\n",
            "episode 214, reward 220, memory_length 2000, epsilon 0.8995404162604025\n",
            "episode 215, reward 40, memory_length 2000, epsilon 0.8990957572262945\n",
            "episode 216, reward -38, memory_length 2000, epsilon 0.8986513204661305\n",
            "episode 217, reward 134, memory_length 2000, epsilon 0.8982071058688013\n",
            "episode 218, reward 191, memory_length 2000, epsilon 0.8977631133232531\n",
            "episode 219, reward -42, memory_length 2000, epsilon 0.8973193427184879\n",
            "episode 220, reward 144, memory_length 2000, epsilon 0.896875793943563\n",
            "episode 221, reward -70, memory_length 2000, epsilon 0.8964324668875912\n",
            "episode 222, reward 132, memory_length 2000, epsilon 0.8959893614397407\n",
            "episode 223, reward -115, memory_length 2000, epsilon 0.8955464774892351\n",
            "episode 224, reward -164, memory_length 2000, epsilon 0.8951038149253536\n",
            "episode 225, reward -74, memory_length 2000, epsilon 0.8946613736374305\n",
            "episode 226, reward 44, memory_length 2000, epsilon 0.8942191535148554\n",
            "episode 227, reward -115, memory_length 2000, epsilon 0.8937771544470732\n",
            "episode 228, reward 35, memory_length 2000, epsilon 0.8933353763235842\n",
            "episode 229, reward -183, memory_length 2000, epsilon 0.8928938190339439\n",
            "episode 230, reward 145, memory_length 2000, epsilon 0.892452482467763\n",
            "episode 231, reward 129, memory_length 2000, epsilon 0.8920113665147074\n",
            "episode 232, reward 44, memory_length 2000, epsilon 0.8915704710644979\n",
            "episode 233, reward -129, memory_length 2000, epsilon 0.8911297960069108\n",
            "episode 234, reward 49, memory_length 2000, epsilon 0.8906893412317772\n",
            "episode 235, reward 24, memory_length 2000, epsilon 0.8902491066289836\n",
            "episode 236, reward -352, memory_length 2000, epsilon 0.8898090920884711\n",
            "episode 237, reward 0, memory_length 2000, epsilon 0.8893692975002364\n",
            "episode 238, reward 49, memory_length 2000, epsilon 0.8889297227543306\n",
            "episode 239, reward -239, memory_length 2000, epsilon 0.88849036774086\n",
            "episode 240, reward -131, memory_length 2000, epsilon 0.888051232349986\n",
            "episode 241, reward 49, memory_length 2000, epsilon 0.8876123164719245\n",
            "episode 242, reward -94, memory_length 2000, epsilon 0.8871736199969468\n",
            "episode 243, reward -22, memory_length 2000, epsilon 0.8867351428153787\n",
            "episode 244, reward -176, memory_length 2000, epsilon 0.8862968848176008\n",
            "episode 245, reward -56, memory_length 2000, epsilon 0.8858588458940487\n",
            "episode 246, reward -80, memory_length 2000, epsilon 0.8854210259352127\n",
            "episode 247, reward -244, memory_length 2000, epsilon 0.8849834248316377\n",
            "episode 248, reward -23, memory_length 2000, epsilon 0.8845460424739234\n",
            "episode 249, reward 50, memory_length 2000, epsilon 0.8841088787527244\n",
            "episode 250, reward -34, memory_length 2000, epsilon 0.8836719335587495\n",
            "episode 251, reward 103, memory_length 2000, epsilon 0.8832352067827626\n",
            "episode 252, reward 53, memory_length 2000, epsilon 0.8827986983155819\n",
            "episode 253, reward -59, memory_length 2000, epsilon 0.8823624080480803\n",
            "episode 254, reward -122, memory_length 2000, epsilon 0.8819263358711854\n",
            "episode 255, reward -265, memory_length 2000, epsilon 0.881490481675879\n",
            "episode 256, reward -213, memory_length 2000, epsilon 0.8810548453531973\n",
            "episode 257, reward -15, memory_length 2000, epsilon 0.8806194267942318\n",
            "episode 258, reward -86, memory_length 2000, epsilon 0.8801842258901273\n",
            "episode 259, reward -126, memory_length 2000, epsilon 0.879749242532084\n",
            "episode 260, reward -326, memory_length 2000, epsilon 0.8793144766113556\n",
            "episode 261, reward -13, memory_length 2000, epsilon 0.8788799280192511\n",
            "episode 262, reward 67, memory_length 2000, epsilon 0.8784455966471331\n",
            "episode 263, reward 4, memory_length 2000, epsilon 0.8780114823864188\n",
            "episode 264, reward 66, memory_length 2000, epsilon 0.8775775851285795\n",
            "episode 265, reward 40, memory_length 2000, epsilon 0.8771439047651411\n",
            "episode 266, reward -22, memory_length 2000, epsilon 0.8767104411876834\n",
            "episode 267, reward 489, memory_length 2000, epsilon 0.8762771942878405\n",
            "episode 268, reward -129, memory_length 2000, epsilon 0.8758441639573007\n",
            "episode 269, reward -311, memory_length 2000, epsilon 0.8754113500878064\n",
            "episode 270, reward -2, memory_length 2000, epsilon 0.8749787525711541\n",
            "episode 271, reward 17, memory_length 2000, epsilon 0.8745463712991944\n",
            "episode 272, reward -75, memory_length 2000, epsilon 0.8741142061638321\n",
            "episode 273, reward 105, memory_length 2000, epsilon 0.8736822570570258\n",
            "episode 274, reward 42, memory_length 2000, epsilon 0.8732505238707883\n",
            "episode 275, reward -70, memory_length 2000, epsilon 0.8728190064971862\n",
            "episode 276, reward 501, memory_length 2000, epsilon 0.8723877048283404\n",
            "episode 277, reward -228, memory_length 2000, epsilon 0.871956618756425\n",
            "episode 278, reward 32, memory_length 2000, epsilon 0.871525748173669\n",
            "episode 279, reward -160, memory_length 2000, epsilon 0.8710950929723545\n",
            "episode 280, reward -194, memory_length 2000, epsilon 0.8706646530448178\n",
            "episode 281, reward -353, memory_length 2000, epsilon 0.8702344282834488\n",
            "episode 282, reward 100, memory_length 2000, epsilon 0.8698044185806911\n",
            "episode 283, reward 435, memory_length 2000, epsilon 0.8693746238290428\n",
            "episode 284, reward -65, memory_length 2000, epsilon 0.8689450439210549\n",
            "episode 285, reward -33, memory_length 2000, epsilon 0.8685156787493323\n",
            "episode 286, reward -259, memory_length 2000, epsilon 0.868086528206534\n",
            "episode 287, reward 114, memory_length 2000, epsilon 0.8676575921853722\n",
            "episode 288, reward -244, memory_length 2000, epsilon 0.867228870578613\n",
            "episode 289, reward 58, memory_length 2000, epsilon 0.8668003632790758\n",
            "episode 290, reward -311, memory_length 2000, epsilon 0.8663720701796339\n",
            "episode 291, reward 116, memory_length 2000, epsilon 0.865943991173214\n",
            "episode 292, reward -256, memory_length 2000, epsilon 0.8655161261527964\n",
            "episode 293, reward 317, memory_length 2000, epsilon 0.8650884750114147\n",
            "episode 294, reward 49, memory_length 2000, epsilon 0.8646610376421562\n",
            "episode 295, reward 55, memory_length 2000, epsilon 0.8642338139381617\n",
            "episode 296, reward 84, memory_length 2000, epsilon 0.8638068037926251\n",
            "episode 297, reward -44, memory_length 2000, epsilon 0.8633800070987937\n",
            "episode 298, reward -31, memory_length 2000, epsilon 0.8629534237499686\n",
            "episode 299, reward 25, memory_length 2000, epsilon 0.862527053639504\n",
            "episode 300, reward -181, memory_length 2000, epsilon 0.8621008966608072\n",
            "episode 301, reward 29, memory_length 2000, epsilon 0.861674952707339\n",
            "episode 302, reward 105, memory_length 2000, epsilon 0.8612492216726134\n",
            "episode 303, reward 176, memory_length 2000, epsilon 0.8608237034501977\n",
            "episode 304, reward -314, memory_length 2000, epsilon 0.8603983979337122\n",
            "episode 305, reward -6, memory_length 2000, epsilon 0.8599733050168307\n",
            "episode 306, reward 25, memory_length 2000, epsilon 0.8595484245932798\n",
            "episode 307, reward -4, memory_length 2000, epsilon 0.8591237565568396\n",
            "episode 308, reward 304, memory_length 2000, epsilon 0.8586993008013429\n",
            "episode 309, reward 139, memory_length 2000, epsilon 0.8582750572206758\n",
            "episode 310, reward -28, memory_length 2000, epsilon 0.8578510257087774\n",
            "episode 311, reward -140, memory_length 2000, epsilon 0.8574272061596399\n",
            "episode 312, reward -301, memory_length 2000, epsilon 0.8570035984673082\n",
            "episode 313, reward -297, memory_length 2000, epsilon 0.8565802025258807\n",
            "episode 314, reward 241, memory_length 2000, epsilon 0.8561570182295082\n",
            "episode 315, reward 44, memory_length 2000, epsilon 0.8557340454723947\n",
            "episode 316, reward -287, memory_length 2000, epsilon 0.8553112841487969\n",
            "episode 317, reward -197, memory_length 2000, epsilon 0.8548887341530246\n",
            "episode 318, reward -20, memory_length 2000, epsilon 0.8544663953794402\n",
            "episode 319, reward 49, memory_length 2000, epsilon 0.8540442677224591\n",
            "episode 320, reward -320, memory_length 2000, epsilon 0.8536223510765493\n",
            "episode 321, reward -37, memory_length 2000, epsilon 0.8532006453362315\n",
            "episode 322, reward -176, memory_length 2000, epsilon 0.8527791503960795\n",
            "episode 323, reward 127, memory_length 2000, epsilon 0.8523578661507196\n",
            "episode 324, reward -209, memory_length 2000, epsilon 0.8519367924948307\n",
            "episode 325, reward 15, memory_length 2000, epsilon 0.8515159293231441\n",
            "episode 326, reward -79, memory_length 2000, epsilon 0.8510952765304444\n",
            "episode 327, reward 292, memory_length 2000, epsilon 0.8506748340115681\n",
            "episode 328, reward -78, memory_length 2000, epsilon 0.8502546016614047\n",
            "episode 329, reward -230, memory_length 2000, epsilon 0.849834579374896\n",
            "episode 330, reward 178, memory_length 2000, epsilon 0.8494147670470367\n",
            "episode 331, reward 102, memory_length 2000, epsilon 0.8489951645728735\n",
            "episode 332, reward 60, memory_length 2000, epsilon 0.8485757718475057\n",
            "episode 333, reward -221, memory_length 2000, epsilon 0.8481565887660852\n",
            "episode 334, reward 134, memory_length 2000, epsilon 0.8477376152238164\n",
            "episode 335, reward -73, memory_length 2000, epsilon 0.8473188511159557\n",
            "episode 336, reward -50, memory_length 2000, epsilon 0.8469002963378122\n",
            "episode 337, reward 8, memory_length 2000, epsilon 0.8464819507847471\n",
            "episode 338, reward -179, memory_length 2000, epsilon 0.846063814352174\n",
            "episode 339, reward 82, memory_length 2000, epsilon 0.8456458869355589\n",
            "episode 340, reward -178, memory_length 2000, epsilon 0.8452281684304199\n",
            "episode 341, reward 8, memory_length 2000, epsilon 0.8448106587323273\n",
            "episode 342, reward 384, memory_length 2000, epsilon 0.8443933577369037\n",
            "episode 343, reward -117, memory_length 2000, epsilon 0.843976265339824\n",
            "episode 344, reward -159, memory_length 2000, epsilon 0.8435593814368149\n",
            "episode 345, reward 170, memory_length 2000, epsilon 0.8431427059236555\n",
            "episode 346, reward -111, memory_length 2000, epsilon 0.842726238696177\n",
            "episode 347, reward -176, memory_length 2000, epsilon 0.8423099796502623\n",
            "episode 348, reward -344, memory_length 2000, epsilon 0.8418939286818471\n",
            "episode 349, reward -185, memory_length 2000, epsilon 0.8414780856869183\n",
            "episode 350, reward 23, memory_length 2000, epsilon 0.8410624505615153\n",
            "episode 351, reward -39, memory_length 2000, epsilon 0.8406470232017291\n",
            "episode 352, reward -288, memory_length 2000, epsilon 0.8402318035037033\n",
            "episode 353, reward -182, memory_length 2000, epsilon 0.8398167913636325\n",
            "episode 354, reward -492, memory_length 2000, epsilon 0.8394019866777639\n",
            "episode 355, reward -51, memory_length 2000, epsilon 0.8389873893423964\n",
            "episode 356, reward 80, memory_length 2000, epsilon 0.8385729992538804\n",
            "episode 357, reward -323, memory_length 2000, epsilon 0.8381588163086185\n",
            "episode 358, reward -192, memory_length 2000, epsilon 0.8377448404030652\n",
            "episode 359, reward 145, memory_length 2000, epsilon 0.8373310714337262\n",
            "episode 360, reward -7, memory_length 2000, epsilon 0.8369175092971592\n",
            "episode 361, reward 191, memory_length 2000, epsilon 0.8365041538899741\n",
            "episode 362, reward -78, memory_length 2000, epsilon 0.8360910051088316\n",
            "episode 363, reward 31, memory_length 2000, epsilon 0.8356780628504448\n",
            "episode 364, reward -254, memory_length 2000, epsilon 0.835265327011578\n",
            "episode 365, reward 355, memory_length 2000, epsilon 0.8348527974890472\n",
            "episode 366, reward 227, memory_length 2000, epsilon 0.83444047417972\n",
            "episode 367, reward -143, memory_length 2000, epsilon 0.8340283569805158\n",
            "episode 368, reward -164, memory_length 2000, epsilon 0.833616445788405\n",
            "episode 369, reward 109, memory_length 2000, epsilon 0.83320474050041\n",
            "episode 370, reward -217, memory_length 2000, epsilon 0.8327932410136044\n",
            "episode 371, reward 157, memory_length 2000, epsilon 0.8323819472251133\n",
            "episode 372, reward 82, memory_length 2000, epsilon 0.8319708590321133\n",
            "episode 373, reward 206, memory_length 2000, epsilon 0.8315599763318324\n",
            "episode 374, reward -159, memory_length 2000, epsilon 0.8311492990215499\n",
            "episode 375, reward 220, memory_length 2000, epsilon 0.8307388269985964\n",
            "episode 376, reward 341, memory_length 2000, epsilon 0.8303285601603538\n",
            "episode 377, reward -37, memory_length 2000, epsilon 0.8299184984042556\n",
            "episode 378, reward 175, memory_length 2000, epsilon 0.8295086416277863\n",
            "episode 379, reward 52, memory_length 2000, epsilon 0.8290989897284816\n",
            "episode 380, reward -241, memory_length 2000, epsilon 0.8286895426039287\n",
            "episode 381, reward 48, memory_length 2000, epsilon 0.8282803001517657\n",
            "episode 382, reward 43, memory_length 2000, epsilon 0.8278712622696819\n",
            "episode 383, reward -185, memory_length 2000, epsilon 0.827462428855418\n",
            "episode 384, reward -188, memory_length 2000, epsilon 0.8270537998067655\n",
            "episode 385, reward 5, memory_length 2000, epsilon 0.8266453750215673\n",
            "episode 386, reward -101, memory_length 2000, epsilon 0.826237154397717\n",
            "episode 387, reward -97, memory_length 2000, epsilon 0.8258291378331598\n",
            "episode 388, reward -226, memory_length 2000, epsilon 0.8254213252258911\n",
            "episode 389, reward 94, memory_length 2000, epsilon 0.8250137164739579\n",
            "episode 390, reward -285, memory_length 2000, epsilon 0.8246063114754583\n",
            "episode 391, reward -24, memory_length 2000, epsilon 0.8241991101285405\n",
            "episode 392, reward 58, memory_length 2000, epsilon 0.8237921123314047\n",
            "episode 393, reward 246, memory_length 2000, epsilon 0.8233853179823012\n",
            "episode 394, reward -8, memory_length 2000, epsilon 0.8229787269795313\n",
            "episode 395, reward 7, memory_length 2000, epsilon 0.8225723392214475\n",
            "episode 396, reward -381, memory_length 2000, epsilon 0.8221661546064527\n",
            "episode 397, reward -158, memory_length 2000, epsilon 0.8217601730330008\n",
            "episode 398, reward -5, memory_length 2000, epsilon 0.8213543943995963\n",
            "episode 399, reward -78, memory_length 2000, epsilon 0.8209488186047948\n",
            "episode 400, reward 223, memory_length 2000, epsilon 0.820543445547202\n",
            "episode 401, reward 209, memory_length 2000, epsilon 0.8201382751254749\n",
            "episode 402, reward -240, memory_length 2000, epsilon 0.8197333072383208\n",
            "episode 403, reward -24, memory_length 2000, epsilon 0.8193285417844978\n",
            "episode 404, reward 26, memory_length 2000, epsilon 0.8189239786628142\n",
            "episode 405, reward 75, memory_length 2000, epsilon 0.8185196177721297\n",
            "episode 406, reward -244, memory_length 2000, epsilon 0.8181154590113539\n",
            "episode 407, reward -573, memory_length 2000, epsilon 0.8177115022794469\n",
            "episode 408, reward -20, memory_length 2000, epsilon 0.8173077474754198\n",
            "episode 409, reward -26, memory_length 2000, epsilon 0.8169041944983336\n",
            "episode 410, reward 89, memory_length 2000, epsilon 0.8165008432473004\n",
            "episode 411, reward 315, memory_length 2000, epsilon 0.8160976936214821\n",
            "episode 412, reward -255, memory_length 2000, epsilon 0.8156947455200915\n",
            "episode 413, reward 136, memory_length 2000, epsilon 0.8152919988423915\n",
            "episode 414, reward 107, memory_length 2000, epsilon 0.8148894534876954\n",
            "episode 415, reward -226, memory_length 2000, epsilon 0.8144871093553667\n",
            "episode 416, reward -218, memory_length 2000, epsilon 0.8140849663448195\n",
            "episode 417, reward -94, memory_length 2000, epsilon 0.8136830243555183\n",
            "episode 418, reward 147, memory_length 2000, epsilon 0.8132812832869774\n",
            "episode 419, reward 97, memory_length 2000, epsilon 0.8128797430387613\n",
            "episode 420, reward -136, memory_length 2000, epsilon 0.8124784035104852\n",
            "episode 421, reward 22, memory_length 2000, epsilon 0.8120772646018142\n",
            "episode 422, reward -102, memory_length 2000, epsilon 0.8116763262124636\n",
            "episode 423, reward -226, memory_length 2000, epsilon 0.8112755882421986\n",
            "episode 424, reward -293, memory_length 2000, epsilon 0.8108750505908348\n",
            "episode 425, reward 110, memory_length 2000, epsilon 0.8104747131582379\n",
            "episode 426, reward -145, memory_length 2000, epsilon 0.8100745758443235\n",
            "episode 427, reward -142, memory_length 2000, epsilon 0.8096746385490572\n",
            "episode 428, reward -31, memory_length 2000, epsilon 0.8092749011724547\n",
            "episode 429, reward 50, memory_length 2000, epsilon 0.8088753636145816\n",
            "episode 430, reward 232, memory_length 2000, epsilon 0.8084760257755536\n",
            "episode 431, reward 250, memory_length 2000, epsilon 0.8080768875555362\n",
            "episode 432, reward 146, memory_length 2000, epsilon 0.8076779488547449\n",
            "episode 433, reward -320, memory_length 2000, epsilon 0.8072792095734449\n",
            "episode 434, reward 74, memory_length 2000, epsilon 0.8068806696119515\n",
            "episode 435, reward -365, memory_length 2000, epsilon 0.8064823288706297\n",
            "episode 436, reward 15, memory_length 2000, epsilon 0.8060841872498941\n",
            "episode 437, reward -132, memory_length 2000, epsilon 0.8056862446502095\n",
            "episode 438, reward -30, memory_length 2000, epsilon 0.8052885009720903\n",
            "episode 439, reward 256, memory_length 2000, epsilon 0.8048909561161004\n",
            "episode 440, reward -41, memory_length 2000, epsilon 0.8044936099828537\n",
            "episode 441, reward -146, memory_length 2000, epsilon 0.8040964624730136\n",
            "episode 442, reward -156, memory_length 2000, epsilon 0.8036995134872933\n",
            "episode 443, reward 111, memory_length 2000, epsilon 0.8033027629264555\n",
            "episode 444, reward 270, memory_length 2000, epsilon 0.8029062106913125\n",
            "episode 445, reward 2, memory_length 2000, epsilon 0.8025098566827266\n",
            "episode 446, reward -47, memory_length 2000, epsilon 0.8021137008016086\n",
            "episode 447, reward -208, memory_length 2000, epsilon 0.8017177429489201\n",
            "episode 448, reward 99, memory_length 2000, epsilon 0.8013219830256715\n",
            "episode 449, reward 30, memory_length 2000, epsilon 0.8009264209329227\n",
            "episode 450, reward 80, memory_length 2000, epsilon 0.8005310565717833\n",
            "episode 451, reward 42, memory_length 2000, epsilon 0.8001358898434121\n",
            "episode 452, reward -307, memory_length 2000, epsilon 0.7997409206490175\n",
            "episode 453, reward 508, memory_length 2000, epsilon 0.7993461488898572\n",
            "episode 454, reward -191, memory_length 2000, epsilon 0.7989515744672382\n",
            "episode 455, reward 146, memory_length 2000, epsilon 0.798557197282517\n",
            "episode 456, reward 78, memory_length 2000, epsilon 0.7981630172370993\n",
            "episode 457, reward 85, memory_length 2000, epsilon 0.7977690342324398\n",
            "episode 458, reward 94, memory_length 2000, epsilon 0.797375248170043\n",
            "episode 459, reward -70, memory_length 2000, epsilon 0.7969816589514624\n",
            "episode 460, reward 154, memory_length 2000, epsilon 0.7965882664783007\n",
            "episode 461, reward 114, memory_length 2000, epsilon 0.7961950706522096\n",
            "episode 462, reward 39, memory_length 2000, epsilon 0.7958020713748902\n",
            "episode 463, reward -101, memory_length 2000, epsilon 0.7954092685480929\n",
            "episode 464, reward -246, memory_length 2000, epsilon 0.7950166620736169\n",
            "episode 465, reward 61, memory_length 2000, epsilon 0.7946242518533103\n",
            "episode 466, reward 493, memory_length 2000, epsilon 0.7942320377890709\n",
            "episode 467, reward 4, memory_length 2000, epsilon 0.7938400197828449\n",
            "episode 468, reward -24, memory_length 2000, epsilon 0.793448197736628\n",
            "episode 469, reward -12, memory_length 2000, epsilon 0.7930565715524647\n",
            "episode 470, reward -24, memory_length 2000, epsilon 0.7926651411324482\n",
            "episode 471, reward -12, memory_length 2000, epsilon 0.7922739063787212\n",
            "episode 472, reward 116, memory_length 2000, epsilon 0.7918828671934747\n",
            "episode 473, reward -105, memory_length 2000, epsilon 0.7914920234789491\n",
            "episode 474, reward 178, memory_length 2000, epsilon 0.7911013751374335\n",
            "episode 475, reward -94, memory_length 2000, epsilon 0.7907109220712659\n",
            "episode 476, reward -37, memory_length 2000, epsilon 0.7903206641828326\n",
            "episode 477, reward 202, memory_length 2000, epsilon 0.7899306013745697\n",
            "episode 478, reward 85, memory_length 2000, epsilon 0.789540733548961\n",
            "episode 479, reward -167, memory_length 2000, epsilon 0.7891510606085399\n",
            "episode 480, reward -280, memory_length 2000, epsilon 0.7887615824558879\n",
            "episode 481, reward -258, memory_length 2000, epsilon 0.7883722989936356\n",
            "episode 482, reward -213, memory_length 2000, epsilon 0.787983210124462\n",
            "episode 483, reward 106, memory_length 2000, epsilon 0.7875943157510951\n",
            "episode 484, reward -75, memory_length 2000, epsilon 0.7872056157763112\n",
            "episode 485, reward -176, memory_length 2000, epsilon 0.7868171101029353\n",
            "episode 486, reward 232, memory_length 2000, epsilon 0.7864287986338409\n",
            "episode 487, reward -195, memory_length 2000, epsilon 0.7860406812719501\n",
            "episode 488, reward -235, memory_length 2000, epsilon 0.7856527579202339\n",
            "episode 489, reward -117, memory_length 2000, epsilon 0.7852650284817111\n",
            "episode 490, reward 166, memory_length 2000, epsilon 0.7848774928594494\n",
            "episode 491, reward -274, memory_length 2000, epsilon 0.7844901509565652\n",
            "episode 492, reward 34, memory_length 2000, epsilon 0.7841030026762225\n",
            "episode 493, reward -291, memory_length 2000, epsilon 0.7837160479216346\n",
            "episode 494, reward 54, memory_length 2000, epsilon 0.7833292865960627\n",
            "episode 495, reward -175, memory_length 2000, epsilon 0.7829427186028165\n",
            "episode 496, reward -57, memory_length 2000, epsilon 0.782556343845254\n",
            "episode 497, reward -39, memory_length 2000, epsilon 0.7821701622267814\n",
            "episode 498, reward 32, memory_length 2000, epsilon 0.7817841736508534\n",
            "episode 499, reward 170, memory_length 2000, epsilon 0.7813983780209728\n",
            "episode 500, reward -231, memory_length 2000, epsilon 0.7810127752406908\n",
            "episode 501, reward 85, memory_length 2000, epsilon 0.7806273652136066\n",
            "episode 502, reward 16, memory_length 2000, epsilon 0.7802421478433677\n",
            "episode 503, reward -164, memory_length 2000, epsilon 0.7798571230336698\n",
            "episode 504, reward 99, memory_length 2000, epsilon 0.7794722906882566\n",
            "episode 505, reward -113, memory_length 2000, epsilon 0.7790876507109202\n",
            "episode 506, reward 56, memory_length 2000, epsilon 0.7787032030055004\n",
            "episode 507, reward -150, memory_length 2000, epsilon 0.7783189474758854\n",
            "episode 508, reward -93, memory_length 2000, epsilon 0.7779348840260113\n",
            "episode 509, reward 220, memory_length 2000, epsilon 0.777551012559862\n",
            "episode 510, reward 69, memory_length 2000, epsilon 0.7771673329814701\n",
            "episode 511, reward -250, memory_length 2000, epsilon 0.7767838451949154\n",
            "episode 512, reward -61, memory_length 2000, epsilon 0.7764005491043259\n",
            "episode 513, reward -147, memory_length 2000, epsilon 0.7760174446138777\n",
            "episode 514, reward 214, memory_length 2000, epsilon 0.7756345316277946\n",
            "episode 515, reward -188, memory_length 2000, epsilon 0.7752518100503484\n",
            "episode 516, reward -55, memory_length 2000, epsilon 0.7748692797858587\n",
            "episode 517, reward -335, memory_length 2000, epsilon 0.774486940738693\n",
            "episode 518, reward -120, memory_length 2000, epsilon 0.7741047928132664\n",
            "episode 519, reward 115, memory_length 2000, epsilon 0.773722835914042\n",
            "episode 520, reward 111, memory_length 2000, epsilon 0.7733410699455306\n",
            "episode 521, reward 41, memory_length 2000, epsilon 0.7729594948122906\n",
            "episode 522, reward -192, memory_length 2000, epsilon 0.7725781104189283\n",
            "episode 523, reward -101, memory_length 2000, epsilon 0.7721969166700976\n",
            "episode 524, reward -23, memory_length 2000, epsilon 0.7718159134705\n",
            "episode 525, reward 152, memory_length 2000, epsilon 0.7714351007248849\n",
            "episode 526, reward -135, memory_length 2000, epsilon 0.7710544783380486\n",
            "episode 527, reward 214, memory_length 2000, epsilon 0.7706740462148362\n",
            "episode 528, reward -152, memory_length 2000, epsilon 0.770293804260139\n",
            "episode 529, reward 334, memory_length 2000, epsilon 0.7699137523788971\n",
            "episode 530, reward -81, memory_length 2000, epsilon 0.7695338904760971\n",
            "episode 531, reward 125, memory_length 2000, epsilon 0.7691542184567738\n",
            "episode 532, reward 278, memory_length 2000, epsilon 0.768774736226009\n",
            "episode 533, reward -68, memory_length 2000, epsilon 0.7683954436889322\n",
            "episode 534, reward -48, memory_length 2000, epsilon 0.7680163407507201\n",
            "episode 535, reward 7, memory_length 2000, epsilon 0.7676374273165975\n",
            "episode 536, reward -114, memory_length 2000, epsilon 0.7672587032918353\n",
            "episode 537, reward -94, memory_length 2000, epsilon 0.7668801685817531\n",
            "episode 538, reward -98, memory_length 2000, epsilon 0.766501823091717\n",
            "episode 539, reward -294, memory_length 2000, epsilon 0.7661236667271405\n",
            "episode 540, reward 94, memory_length 2000, epsilon 0.7657456993934846\n",
            "episode 541, reward 92, memory_length 2000, epsilon 0.7653679209962576\n",
            "episode 542, reward -98, memory_length 2000, epsilon 0.7649903314410147\n",
            "episode 543, reward -154, memory_length 2000, epsilon 0.7646129306333587\n",
            "episode 544, reward 292, memory_length 2000, epsilon 0.7642357184789392\n",
            "episode 545, reward -32, memory_length 2000, epsilon 0.7638586948834531\n",
            "episode 546, reward -11, memory_length 2000, epsilon 0.7634818597526449\n",
            "episode 547, reward 148, memory_length 2000, epsilon 0.7631052129923055\n",
            "episode 548, reward -71, memory_length 2000, epsilon 0.7627287545082733\n",
            "episode 549, reward -422, memory_length 2000, epsilon 0.7623524842064335\n",
            "episode 550, reward 155, memory_length 2000, epsilon 0.7619764019927188\n",
            "episode 551, reward 3, memory_length 2000, epsilon 0.7616005077731085\n",
            "episode 552, reward 511, memory_length 2000, epsilon 0.7612248014536289\n",
            "episode 553, reward -163, memory_length 2000, epsilon 0.7608492829403538\n",
            "episode 554, reward 148, memory_length 2000, epsilon 0.7604739521394033\n",
            "episode 555, reward 117, memory_length 2000, epsilon 0.7600988089569446\n",
            "episode 556, reward -242, memory_length 2000, epsilon 0.7597238532991921\n",
            "episode 557, reward -2, memory_length 2000, epsilon 0.759349085072407\n",
            "episode 558, reward -208, memory_length 2000, epsilon 0.7589745041828969\n",
            "episode 559, reward 595, memory_length 2000, epsilon 0.7586001105370167\n",
            "episode 560, reward -244, memory_length 2000, epsilon 0.7582259040411682\n",
            "episode 561, reward 121, memory_length 2000, epsilon 0.7578518846017995\n",
            "episode 562, reward 210, memory_length 2000, epsilon 0.7574780521254059\n",
            "episode 563, reward 43, memory_length 2000, epsilon 0.7571044065185292\n",
            "episode 564, reward -39, memory_length 2000, epsilon 0.7567309476877581\n",
            "episode 565, reward -99, memory_length 2000, epsilon 0.7563576755397277\n",
            "episode 566, reward 143, memory_length 2000, epsilon 0.7559845899811201\n",
            "episode 567, reward 75, memory_length 2000, epsilon 0.7556116909186639\n",
            "episode 568, reward -39, memory_length 2000, epsilon 0.7552389782591343\n",
            "episode 569, reward -170, memory_length 2000, epsilon 0.7548664519093531\n",
            "episode 570, reward 211, memory_length 2000, epsilon 0.7544941117761887\n",
            "episode 571, reward 0, memory_length 2000, epsilon 0.7541219577665563\n",
            "episode 572, reward -193, memory_length 2000, epsilon 0.7537499897874171\n",
            "episode 573, reward 187, memory_length 2000, epsilon 0.7533782077457792\n",
            "episode 574, reward -30, memory_length 2000, epsilon 0.7530066115486973\n",
            "episode 575, reward -195, memory_length 2000, epsilon 0.752635201103272\n",
            "episode 576, reward 32, memory_length 2000, epsilon 0.7522639763166509\n",
            "episode 577, reward 359, memory_length 2000, epsilon 0.7518929370960278\n",
            "episode 578, reward 206, memory_length 2000, epsilon 0.7515220833486427\n",
            "episode 579, reward -349, memory_length 2000, epsilon 0.7511514149817824\n",
            "episode 580, reward 49, memory_length 2000, epsilon 0.7507809319027796\n",
            "episode 581, reward -29, memory_length 2000, epsilon 0.7504106340190136\n",
            "episode 582, reward -68, memory_length 2000, epsilon 0.7500405212379101\n",
            "episode 583, reward -298, memory_length 2000, epsilon 0.7496705934669406\n",
            "episode 584, reward -68, memory_length 2000, epsilon 0.7493008506136236\n",
            "episode 585, reward 108, memory_length 2000, epsilon 0.7489312925855229\n",
            "episode 586, reward -113, memory_length 2000, epsilon 0.7485619192902493\n",
            "episode 587, reward 116, memory_length 2000, epsilon 0.7481927306354593\n",
            "episode 588, reward 205, memory_length 2000, epsilon 0.7478237265288558\n",
            "episode 589, reward 132, memory_length 2000, epsilon 0.7474549068781877\n",
            "episode 590, reward -50, memory_length 2000, epsilon 0.7470862715912503\n",
            "episode 591, reward -337, memory_length 2000, epsilon 0.7467178205758846\n",
            "episode 592, reward -136, memory_length 2000, epsilon 0.7463495537399778\n",
            "episode 593, reward 363, memory_length 2000, epsilon 0.7459814709914633\n",
            "episode 594, reward -218, memory_length 2000, epsilon 0.7456135722383205\n",
            "episode 595, reward 176, memory_length 2000, epsilon 0.7452458573885745\n",
            "episode 596, reward 211, memory_length 2000, epsilon 0.7448783263502966\n",
            "episode 597, reward -140, memory_length 2000, epsilon 0.7445109790316042\n",
            "episode 598, reward 103, memory_length 2000, epsilon 0.7441438153406604\n",
            "episode 599, reward -124, memory_length 2000, epsilon 0.7437768351856743\n",
            "episode 600, reward 380, memory_length 2000, epsilon 0.7434100384749007\n",
            "episode 601, reward 107, memory_length 2000, epsilon 0.7430434251166406\n",
            "episode 602, reward 193, memory_length 2000, epsilon 0.7426769950192406\n",
            "episode 603, reward 140, memory_length 2000, epsilon 0.7423107480910931\n",
            "episode 604, reward 86, memory_length 2000, epsilon 0.7419446842406366\n",
            "episode 605, reward 99, memory_length 2000, epsilon 0.7415788033763548\n",
            "episode 606, reward -217, memory_length 2000, epsilon 0.7412131054067778\n",
            "episode 607, reward -204, memory_length 2000, epsilon 0.7408475902404807\n",
            "episode 608, reward 273, memory_length 2000, epsilon 0.7404822577860852\n",
            "episode 609, reward 30, memory_length 2000, epsilon 0.7401171079522579\n",
            "episode 610, reward 389, memory_length 2000, epsilon 0.7397521406477116\n",
            "episode 611, reward -114, memory_length 2000, epsilon 0.739387355781204\n",
            "episode 612, reward 296, memory_length 2000, epsilon 0.7390227532615391\n",
            "episode 613, reward 84, memory_length 2000, epsilon 0.7386583329975664\n",
            "episode 614, reward 107, memory_length 2000, epsilon 0.7382940948981808\n",
            "episode 615, reward 119, memory_length 2000, epsilon 0.7379300388723227\n",
            "episode 616, reward 168, memory_length 2000, epsilon 0.7375661648289781\n",
            "episode 617, reward -213, memory_length 2000, epsilon 0.7372024726771784\n",
            "episode 618, reward -122, memory_length 2000, epsilon 0.7368389623260008\n",
            "episode 619, reward -1, memory_length 2000, epsilon 0.7364756336845675\n",
            "episode 620, reward -154, memory_length 2000, epsilon 0.7361124866620463\n",
            "episode 621, reward -137, memory_length 2000, epsilon 0.7357495211676507\n",
            "episode 622, reward 128, memory_length 2000, epsilon 0.7353867371106392\n",
            "episode 623, reward -71, memory_length 2000, epsilon 0.7350241344003157\n",
            "episode 624, reward -20, memory_length 2000, epsilon 0.7346617129460296\n",
            "episode 625, reward -62, memory_length 2000, epsilon 0.7342994726571753\n",
            "episode 626, reward -93, memory_length 2000, epsilon 0.7339374134431932\n",
            "episode 627, reward -186, memory_length 2000, epsilon 0.7335755352135681\n",
            "episode 628, reward 46, memory_length 2000, epsilon 0.7332138378778307\n",
            "episode 629, reward 109, memory_length 2000, epsilon 0.7328523213455564\n",
            "episode 630, reward 166, memory_length 2000, epsilon 0.7324909855263663\n",
            "episode 631, reward 113, memory_length 2000, epsilon 0.7321298303299262\n",
            "episode 632, reward 63, memory_length 2000, epsilon 0.7317688556659475\n",
            "episode 633, reward 263, memory_length 2000, epsilon 0.7314080614441866\n",
            "episode 634, reward 65, memory_length 2000, epsilon 0.7310474475744446\n",
            "episode 635, reward 218, memory_length 2000, epsilon 0.7306870139665684\n",
            "episode 636, reward -92, memory_length 2000, epsilon 0.7303267605304495\n",
            "episode 637, reward -264, memory_length 2000, epsilon 0.7299666871760244\n",
            "episode 638, reward -131, memory_length 2000, epsilon 0.7296067938132749\n",
            "episode 639, reward 40, memory_length 2000, epsilon 0.7292470803522275\n",
            "episode 640, reward 49, memory_length 2000, epsilon 0.7288875467029541\n",
            "episode 641, reward -123, memory_length 2000, epsilon 0.7285281927755709\n",
            "episode 642, reward -37, memory_length 2000, epsilon 0.7281690184802397\n",
            "episode 643, reward -59, memory_length 2000, epsilon 0.727810023727167\n",
            "episode 644, reward -155, memory_length 2000, epsilon 0.7274512084266038\n",
            "episode 645, reward -232, memory_length 2000, epsilon 0.7270925724888465\n",
            "episode 646, reward -114, memory_length 2000, epsilon 0.7267341158242361\n",
            "episode 647, reward -59, memory_length 2000, epsilon 0.7263758383431583\n",
            "episode 648, reward -51, memory_length 2000, epsilon 0.7260177399560439\n",
            "episode 649, reward -87, memory_length 2000, epsilon 0.7256598205733683\n",
            "episode 650, reward -56, memory_length 2000, epsilon 0.7253020801056514\n",
            "episode 651, reward 184, memory_length 2000, epsilon 0.7249445184634584\n",
            "episode 652, reward -170, memory_length 2000, epsilon 0.7245871355573987\n",
            "episode 653, reward 316, memory_length 2000, epsilon 0.7242299312981265\n",
            "episode 654, reward 191, memory_length 2000, epsilon 0.723872905596341\n",
            "episode 655, reward 68, memory_length 2000, epsilon 0.7235160583627855\n",
            "episode 656, reward 290, memory_length 2000, epsilon 0.7231593895082483\n",
            "episode 657, reward -176, memory_length 2000, epsilon 0.7228028989435623\n",
            "episode 658, reward 400, memory_length 2000, epsilon 0.7224465865796047\n",
            "episode 659, reward -329, memory_length 2000, epsilon 0.7220904523272974\n",
            "episode 660, reward 57, memory_length 2000, epsilon 0.7217344960976069\n",
            "episode 661, reward 251, memory_length 2000, epsilon 0.7213787178015442\n",
            "episode 662, reward 616, memory_length 2000, epsilon 0.7210231173501646\n",
            "episode 663, reward 119, memory_length 2000, epsilon 0.720667694654568\n",
            "episode 664, reward -27, memory_length 2000, epsilon 0.7203124496258988\n",
            "episode 665, reward -42, memory_length 2000, epsilon 0.7199573821753458\n",
            "episode 666, reward 164, memory_length 2000, epsilon 0.719602492214142\n",
            "episode 667, reward 115, memory_length 2000, epsilon 0.7192477796535648\n",
            "episode 668, reward -198, memory_length 2000, epsilon 0.7188932444049364\n",
            "episode 669, reward 62, memory_length 2000, epsilon 0.7185388863796227\n",
            "episode 670, reward -5, memory_length 2000, epsilon 0.7181847054890343\n",
            "episode 671, reward 328, memory_length 2000, epsilon 0.7178307016446259\n",
            "episode 672, reward 378, memory_length 2000, epsilon 0.7174768747578967\n",
            "episode 673, reward 146, memory_length 2000, epsilon 0.7171232247403899\n",
            "episode 674, reward 359, memory_length 2000, epsilon 0.7167697515036928\n",
            "episode 675, reward -81, memory_length 2000, epsilon 0.7164164549594374\n",
            "episode 676, reward -88, memory_length 2000, epsilon 0.7160633350192994\n",
            "episode 677, reward 179, memory_length 2000, epsilon 0.7157103915949988\n",
            "episode 678, reward -146, memory_length 2000, epsilon 0.7153576245982999\n",
            "episode 679, reward -241, memory_length 2000, epsilon 0.7150050339410107\n",
            "episode 680, reward 161, memory_length 2000, epsilon 0.7146526195349836\n",
            "episode 681, reward 310, memory_length 2000, epsilon 0.714300381292115\n",
            "episode 682, reward 412, memory_length 2000, epsilon 0.7139483191243456\n",
            "episode 683, reward 335, memory_length 2000, epsilon 0.7135964329436596\n",
            "episode 684, reward 269, memory_length 2000, epsilon 0.7132447226620855\n",
            "episode 685, reward 29, memory_length 2000, epsilon 0.7128931881916957\n",
            "episode 686, reward 121, memory_length 2000, epsilon 0.7125418294446066\n",
            "episode 687, reward 12, memory_length 2000, epsilon 0.7121906463329785\n",
            "episode 688, reward 47, memory_length 2000, epsilon 0.7118396387690156\n",
            "episode 689, reward 160, memory_length 2000, epsilon 0.7114888066649662\n",
            "episode 690, reward -182, memory_length 2000, epsilon 0.711138149933122\n",
            "episode 691, reward -146, memory_length 2000, epsilon 0.7107876684858189\n",
            "episode 692, reward 25, memory_length 2000, epsilon 0.7104373622354366\n",
            "episode 693, reward 202, memory_length 2000, epsilon 0.7100872310943986\n",
            "episode 694, reward 309, memory_length 2000, epsilon 0.7097372749751719\n",
            "episode 695, reward 94, memory_length 2000, epsilon 0.7093874937902678\n",
            "episode 696, reward 94, memory_length 2000, epsilon 0.7090378874522405\n",
            "episode 697, reward -202, memory_length 2000, epsilon 0.7086884558736889\n",
            "episode 698, reward -2, memory_length 2000, epsilon 0.7083391989672547\n",
            "episode 699, reward 193, memory_length 2000, epsilon 0.7079901166456242\n",
            "episode 700, reward 148, memory_length 2000, epsilon 0.7076412088215263\n",
            "episode 701, reward 58, memory_length 2000, epsilon 0.7072924754077343\n",
            "episode 702, reward 118, memory_length 2000, epsilon 0.7069439163170648\n",
            "episode 703, reward 13, memory_length 2000, epsilon 0.7065955314623779\n",
            "episode 704, reward 169, memory_length 2000, epsilon 0.7062473207565775\n",
            "episode 705, reward -2, memory_length 2000, epsilon 0.7058992841126109\n",
            "episode 706, reward 179, memory_length 2000, epsilon 0.7055514214434692\n",
            "episode 707, reward 98, memory_length 2000, epsilon 0.7052037326621862\n",
            "episode 708, reward -263, memory_length 2000, epsilon 0.70485621768184\n",
            "episode 709, reward 163, memory_length 2000, epsilon 0.704508876415552\n",
            "episode 710, reward -14, memory_length 2000, epsilon 0.7041617087764868\n",
            "episode 711, reward 246, memory_length 2000, epsilon 0.7038147146778522\n",
            "episode 712, reward -239, memory_length 2000, epsilon 0.7034678940328999\n",
            "episode 713, reward 233, memory_length 2000, epsilon 0.7031212467549247\n",
            "episode 714, reward -7, memory_length 2000, epsilon 0.702774772757265\n",
            "episode 715, reward -119, memory_length 2000, epsilon 0.7024284719533018\n",
            "episode 716, reward 30, memory_length 2000, epsilon 0.7020823442564604\n",
            "episode 717, reward 107, memory_length 2000, epsilon 0.7017363895802087\n",
            "episode 718, reward -41, memory_length 2000, epsilon 0.7013906078380578\n",
            "episode 719, reward -182, memory_length 2000, epsilon 0.7010449989435626\n",
            "episode 720, reward -41, memory_length 2000, epsilon 0.7006995628103208\n",
            "episode 721, reward 118, memory_length 2000, epsilon 0.7003542993519732\n",
            "episode 722, reward 241, memory_length 2000, epsilon 0.700009208482204\n",
            "episode 723, reward 67, memory_length 2000, epsilon 0.6996642901147405\n",
            "episode 724, reward 126, memory_length 2000, epsilon 0.6993195441633533\n",
            "episode 725, reward 32, memory_length 2000, epsilon 0.6989749705418555\n",
            "episode 726, reward -41, memory_length 2000, epsilon 0.6986305691641042\n",
            "episode 727, reward -345, memory_length 2000, epsilon 0.6982863399439985\n",
            "episode 728, reward 89, memory_length 2000, epsilon 0.6979422827954815\n",
            "episode 729, reward 67, memory_length 2000, epsilon 0.6975983976325387\n",
            "episode 730, reward 177, memory_length 2000, epsilon 0.6972546843691991\n",
            "episode 731, reward 175, memory_length 2000, epsilon 0.696911142919534\n",
            "episode 732, reward 154, memory_length 2000, epsilon 0.6965677731976583\n",
            "episode 733, reward -104, memory_length 2000, epsilon 0.6962245751177294\n",
            "episode 734, reward 247, memory_length 2000, epsilon 0.6958815485939478\n",
            "episode 735, reward 304, memory_length 2000, epsilon 0.695538693540557\n",
            "episode 736, reward 53, memory_length 2000, epsilon 0.695196009871843\n",
            "episode 737, reward -163, memory_length 2000, epsilon 0.6948534975021353\n",
            "episode 738, reward -123, memory_length 2000, epsilon 0.6945111563458054\n",
            "episode 739, reward -40, memory_length 2000, epsilon 0.6941689863172682\n",
            "episode 740, reward -1, memory_length 2000, epsilon 0.6938269873309811\n",
            "episode 741, reward 47, memory_length 2000, epsilon 0.6934851593014444\n",
            "episode 742, reward -128, memory_length 2000, epsilon 0.6931435021432011\n",
            "episode 743, reward -5, memory_length 2000, epsilon 0.692802015770837\n",
            "episode 744, reward 115, memory_length 2000, epsilon 0.6924607000989802\n",
            "episode 745, reward -71, memory_length 2000, epsilon 0.692119555042302\n",
            "episode 746, reward -4, memory_length 2000, epsilon 0.6917785805155162\n",
            "episode 747, reward -310, memory_length 2000, epsilon 0.6914377764333791\n",
            "episode 748, reward -93, memory_length 2000, epsilon 0.6910971427106897\n",
            "episode 749, reward -41, memory_length 2000, epsilon 0.6907566792622893\n",
            "episode 750, reward 158, memory_length 2000, epsilon 0.6904163860030625\n",
            "episode 751, reward -33, memory_length 2000, epsilon 0.6900762628479357\n",
            "episode 752, reward -23, memory_length 2000, epsilon 0.689736309711878\n",
            "episode 753, reward 226, memory_length 2000, epsilon 0.6893965265099014\n",
            "episode 754, reward 160, memory_length 2000, epsilon 0.68905691315706\n",
            "episode 755, reward 57, memory_length 2000, epsilon 0.6887174695684504\n",
            "episode 756, reward 93, memory_length 2000, epsilon 0.6883781956592117\n",
            "episode 757, reward 157, memory_length 2000, epsilon 0.6880390913445252\n",
            "episode 758, reward 327, memory_length 2000, epsilon 0.6877001565396155\n",
            "episode 759, reward 186, memory_length 2000, epsilon 0.6873613911597483\n",
            "episode 760, reward -214, memory_length 2000, epsilon 0.6870227951202322\n",
            "episode 761, reward -121, memory_length 2000, epsilon 0.6866843683364188\n",
            "episode 762, reward -77, memory_length 2000, epsilon 0.6863461107237007\n",
            "episode 763, reward -192, memory_length 2000, epsilon 0.6860080221975139\n",
            "episode 764, reward 271, memory_length 2000, epsilon 0.6856701026733362\n",
            "episode 765, reward -151, memory_length 2000, epsilon 0.6853323520666876\n",
            "episode 766, reward -349, memory_length 2000, epsilon 0.6849947702931306\n",
            "episode 767, reward 23, memory_length 2000, epsilon 0.6846573572682697\n",
            "episode 768, reward 133, memory_length 2000, epsilon 0.6843201129077516\n",
            "episode 769, reward -142, memory_length 2000, epsilon 0.6839830371272653\n",
            "episode 770, reward 396, memory_length 2000, epsilon 0.6836461298425418\n",
            "episode 771, reward -165, memory_length 2000, epsilon 0.6833093909693542\n",
            "episode 772, reward 5, memory_length 2000, epsilon 0.6829728204235178\n",
            "episode 773, reward 155, memory_length 2000, epsilon 0.68263641812089\n",
            "episode 774, reward 53, memory_length 2000, epsilon 0.6823001839773704\n",
            "episode 775, reward -31, memory_length 2000, epsilon 0.6819641179089002\n",
            "episode 776, reward 210, memory_length 2000, epsilon 0.681628219831463\n",
            "episode 777, reward -23, memory_length 2000, epsilon 0.6812924896610842\n",
            "episode 778, reward -356, memory_length 2000, epsilon 0.6809569273138314\n",
            "episode 779, reward 264, memory_length 2000, epsilon 0.6806215327058138\n",
            "episode 780, reward -57, memory_length 2000, epsilon 0.680286305753183\n",
            "episode 781, reward 18, memory_length 2000, epsilon 0.6799512463721321\n",
            "episode 782, reward 217, memory_length 2000, epsilon 0.6796163544788962\n",
            "episode 783, reward 186, memory_length 2000, epsilon 0.6792816299897526\n",
            "episode 784, reward -25, memory_length 2000, epsilon 0.6789470728210197\n",
            "episode 785, reward -50, memory_length 2000, epsilon 0.6786126828890586\n",
            "episode 786, reward -96, memory_length 2000, epsilon 0.6782784601102718\n",
            "episode 787, reward -204, memory_length 2000, epsilon 0.6779444044011036\n",
            "episode 788, reward 203, memory_length 2000, epsilon 0.6776105156780398\n",
            "episode 789, reward 64, memory_length 2000, epsilon 0.6772767938576084\n",
            "episode 790, reward 293, memory_length 2000, epsilon 0.676943238856379\n",
            "episode 791, reward 344, memory_length 2000, epsilon 0.6766098505909627\n",
            "episode 792, reward 121, memory_length 2000, epsilon 0.6762766289780126\n",
            "episode 793, reward 461, memory_length 2000, epsilon 0.6759435739342231\n",
            "episode 794, reward -176, memory_length 2000, epsilon 0.6756106853763306\n",
            "episode 795, reward 38, memory_length 2000, epsilon 0.6752779632211131\n",
            "episode 796, reward 66, memory_length 2000, epsilon 0.6749454073853897\n",
            "episode 797, reward 150, memory_length 2000, epsilon 0.6746130177860216\n",
            "episode 798, reward 173, memory_length 2000, epsilon 0.6742807943399114\n",
            "episode 799, reward 195, memory_length 2000, epsilon 0.6739487369640033\n",
            "episode 800, reward 61, memory_length 2000, epsilon 0.6736168455752829\n",
            "episode 801, reward -204, memory_length 2000, epsilon 0.6732851200907773\n",
            "episode 802, reward -78, memory_length 2000, epsilon 0.6729535604275553\n",
            "episode 803, reward 348, memory_length 2000, epsilon 0.6726221665027267\n",
            "episode 804, reward 452, memory_length 2000, epsilon 0.6722909382334432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-06c6957b563f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# train after each step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# add reward to the total score of this episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b318da61b59c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# predict the target q-values from states s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# target for q-network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1593\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    700\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mgraph_rewrites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_rewrites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0mgraph_rewrite_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_rewrite_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph_rewrites\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_graph_rewrite_configs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2909\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2911\u001b[0;31m       \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_rewrite_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_slack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/optimization_options.py\u001b[0m in \u001b[0;36m_graph_rewrite_configs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_graph_rewrite_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_vectorization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_vectorization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_rewrite_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/options.py\u001b[0m in \u001b[0;36mget_fn\u001b[0;34m(option)\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m       \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfNw_b0F8jWo"
      },
      "source": [
        "## save stuff as pickle\n",
        "import os\n",
        "def save_pickle(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# make directory\n",
        "if not os.path.exists(\"saved_pickle_files\"):\n",
        "    os.mkdir(\"saved_pickle_files\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxFFaUhO8jWr"
      },
      "source": [
        "# save rewards_per_episode\n",
        "save_pickle(rewards_per_episode, \"saved_pickle_files/rewards_per_episode\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AVVCuQ78jWt"
      },
      "source": [
        "### Tracking Convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anY4OrFt8jWt",
        "outputId": "8cc2b1f2-88f9-4020-af93-a8caeca06e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "\n",
        "# plot results\n",
        "with open('saved_pickle_files/rewards_per_episode.pkl', 'rb') as f:\n",
        "    rewards_per_episode = pickle.load(f)\n",
        "\n",
        "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
        "plt.xlabel(\"episode number\")\n",
        "plt.ylabel(\"reward per episode\")\n",
        "\n",
        "# save plots in saved_plots/ directory\n",
        "plt.savefig('rewards.png')\n",
        "\n",
        "print(\"Average reward of last 100 episodes is {0}\".format(np.mean(rewards_per_episode[-100:]))) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average reward of last 100 episodes is 41.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWpy4aAk8jWw"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPzqSgF8jW0"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfDqenlK8jW3"
      },
      "source": [
        "#### Epsilon-decay sample function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOnjxMl8jW3"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "Try building a similar epsilon-decay function for your model.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CHtA_uC8jW4"
      },
      "source": [
        "time = np.arange(0,10000)\n",
        "epsilon = []\n",
        "for i in range(0,10000):\n",
        "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7zz76zz8jW6"
      },
      "source": [
        "plt.plot(time, epsilon)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62JuOwk18jW-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}